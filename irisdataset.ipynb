{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imambucse3/BU/blob/master/irisdataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Cfa2fAueaq",
        "colab_type": "code",
        "outputId": "d90d8b84-7204-480f-b270-66d7aaf48082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"helllo world\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "helllo world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_YwnvevxF6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaZmX-xhxhPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd #Python Data Analysis Library \n",
        "import numpy as np #Python Scientific Library "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAeSe5-AyH1C",
        "colab_type": "code",
        "outputId": "ed20dee4-412f-4fad-8557-c9aa772263b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "new_names = ['sepal_length','sepal_width','petal_length','petal_width','iris_class']\n",
        "dataset = pd.read_csv(url, names=new_names, skiprows=0, delimiter=',')\n",
        "dataset.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            "sepal_length    150 non-null float64\n",
            "sepal_width     150 non-null float64\n",
            "petal_length    150 non-null float64\n",
            "petal_width     150 non-null float64\n",
            "iris_class      150 non-null object\n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-0yOez7ys7s",
        "colab_type": "code",
        "outputId": "40fefdbb-e42d-4088-fef7-666a00b5bb77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "dataset.head(150)#change head with sample and tail"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>iris_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal_length  sepal_width  petal_length  petal_width      iris_class\n",
              "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
              "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
              "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
              "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
              "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
              "..            ...          ...           ...          ...             ...\n",
              "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
              "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
              "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
              "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
              "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
              "\n",
              "[150 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yFy3ZtGzVDp",
        "colab_type": "code",
        "outputId": "c0be2f21-e00f-4045-b2de-10adc77602c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y = dataset['iris_class']\n",
        "x = dataset.drop(['iris_class'], axis=1)\n",
        "\n",
        "print (\"dataset : \",dataset.shape)\n",
        "print (\"x : \",x.shape)\n",
        "print (\"y : \",y.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset :  (150, 5)\n",
            "x :  (150, 4)\n",
            "y :  (150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev_9vqUO0Q9U",
        "colab_type": "code",
        "outputId": "3dd13864-a0b4-48e8-93ef-cd9785a1d619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#one hot encoding\n",
        "y=pd.get_dummies(y)\n",
        "y.sample(150)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iris-setosa</th>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <th>Iris-virginica</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
              "48             1                0               0\n",
              "8              1                0               0\n",
              "129            0                0               1\n",
              "30             1                0               0\n",
              "39             1                0               0\n",
              "..           ...              ...             ...\n",
              "15             1                0               0\n",
              "66             0                1               0\n",
              "145            0                0               1\n",
              "18             1                0               0\n",
              "134            0                0               1\n",
              "\n",
              "[150 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxOnDao_053_",
        "colab_type": "code",
        "outputId": "697d96af-bb80-4a14-fd84-aafd04753806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Selective import Scikit Learn \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate Training and Validation Sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=40) #0.3 data as data test\n",
        "\n",
        "#converting to float 32bit\n",
        "x_train = np.array(x_train).astype(np.float32)\n",
        "x_test  = np.array(x_test).astype(np.float32)\n",
        "y_train = np.array(y_train).astype(np.float32)\n",
        "y_test  = np.array(y_test).astype(np.float32)\n",
        "\n",
        "#print data split for validation\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(110, 4) (110, 3)\n",
            "(40, 4) (40, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqUgF3Ns1i6O",
        "colab_type": "code",
        "outputId": "d3b433cc-6a8f-4b9d-ccb4-9fa031fd5b6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Importing our model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#model initialization\n",
        "Model = MLPClassifier(hidden_layer_sizes=(9,6), max_iter=2000, alpha=0.01, #try change hidden layer\n",
        "                     solver='sgd', verbose=1,  random_state=121) #try verbode=0 to train with out logging\n",
        "#train our model\n",
        "h=Model.fit(x_train,y_train)\n",
        "#use our model to predict\n",
        "y_pred=Model.predict(x_test)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 3.43178920\n",
            "Iteration 2, loss = 3.37330179\n",
            "Iteration 3, loss = 3.29470318\n",
            "Iteration 4, loss = 3.20311782\n",
            "Iteration 5, loss = 3.10288744\n",
            "Iteration 6, loss = 2.99866879\n",
            "Iteration 7, loss = 2.89514708\n",
            "Iteration 8, loss = 2.79390409\n",
            "Iteration 9, loss = 2.69586118\n",
            "Iteration 10, loss = 2.60287909\n",
            "Iteration 11, loss = 2.51596335\n",
            "Iteration 12, loss = 2.43532364\n",
            "Iteration 13, loss = 2.36403962\n",
            "Iteration 14, loss = 2.30252760\n",
            "Iteration 15, loss = 2.25193593\n",
            "Iteration 16, loss = 2.20857144\n",
            "Iteration 17, loss = 2.17201289\n",
            "Iteration 18, loss = 2.14055106\n",
            "Iteration 19, loss = 2.11228041\n",
            "Iteration 20, loss = 2.08635467\n",
            "Iteration 21, loss = 2.06236928\n",
            "Iteration 22, loss = 2.03985392\n",
            "Iteration 23, loss = 2.01872180\n",
            "Iteration 24, loss = 1.99910430\n",
            "Iteration 25, loss = 1.98070200\n",
            "Iteration 26, loss = 1.96326707\n",
            "Iteration 27, loss = 1.94682383\n",
            "Iteration 28, loss = 1.93135127\n",
            "Iteration 29, loss = 1.91678281\n",
            "Iteration 30, loss = 1.90301188\n",
            "Iteration 31, loss = 1.88978412\n",
            "Iteration 32, loss = 1.87698086\n",
            "Iteration 33, loss = 1.86451061\n",
            "Iteration 34, loss = 1.85226045\n",
            "Iteration 35, loss = 1.84015182\n",
            "Iteration 36, loss = 1.82820284\n",
            "Iteration 37, loss = 1.81634220\n",
            "Iteration 38, loss = 1.80455590\n",
            "Iteration 39, loss = 1.79281305\n",
            "Iteration 40, loss = 1.78117526\n",
            "Iteration 41, loss = 1.76973531\n",
            "Iteration 42, loss = 1.75840959\n",
            "Iteration 43, loss = 1.74722757\n",
            "Iteration 44, loss = 1.73620860\n",
            "Iteration 45, loss = 1.72532117\n",
            "Iteration 46, loss = 1.71462849\n",
            "Iteration 47, loss = 1.70408308\n",
            "Iteration 48, loss = 1.69362295\n",
            "Iteration 49, loss = 1.68317571\n",
            "Iteration 50, loss = 1.67260845\n",
            "Iteration 51, loss = 1.66200923\n",
            "Iteration 52, loss = 1.65136955\n",
            "Iteration 53, loss = 1.64067905\n",
            "Iteration 54, loss = 1.62991989\n",
            "Iteration 55, loss = 1.61909274\n",
            "Iteration 56, loss = 1.60821152\n",
            "Iteration 57, loss = 1.59747160\n",
            "Iteration 58, loss = 1.58678725\n",
            "Iteration 59, loss = 1.57621886\n",
            "Iteration 60, loss = 1.56573706\n",
            "Iteration 61, loss = 1.55539467\n",
            "Iteration 62, loss = 1.54518497\n",
            "Iteration 63, loss = 1.53505085\n",
            "Iteration 64, loss = 1.52503176\n",
            "Iteration 65, loss = 1.51524748\n",
            "Iteration 66, loss = 1.50560410\n",
            "Iteration 67, loss = 1.49611803\n",
            "Iteration 68, loss = 1.48683592\n",
            "Iteration 69, loss = 1.47771513\n",
            "Iteration 70, loss = 1.46879887\n",
            "Iteration 71, loss = 1.46003811\n",
            "Iteration 72, loss = 1.45142673\n",
            "Iteration 73, loss = 1.44299064\n",
            "Iteration 74, loss = 1.43479112\n",
            "Iteration 75, loss = 1.42680086\n",
            "Iteration 76, loss = 1.41899189\n",
            "Iteration 77, loss = 1.41134499\n",
            "Iteration 78, loss = 1.40388471\n",
            "Iteration 79, loss = 1.39659025\n",
            "Iteration 80, loss = 1.38945567\n",
            "Iteration 81, loss = 1.38246601\n",
            "Iteration 82, loss = 1.37561300\n",
            "Iteration 83, loss = 1.36888585\n",
            "Iteration 84, loss = 1.36228697\n",
            "Iteration 85, loss = 1.35579531\n",
            "Iteration 86, loss = 1.34939890\n",
            "Iteration 87, loss = 1.34311896\n",
            "Iteration 88, loss = 1.33691470\n",
            "Iteration 89, loss = 1.33080667\n",
            "Iteration 90, loss = 1.32476449\n",
            "Iteration 91, loss = 1.31878203\n",
            "Iteration 92, loss = 1.31286977\n",
            "Iteration 93, loss = 1.30701283\n",
            "Iteration 94, loss = 1.30120249\n",
            "Iteration 95, loss = 1.29544205\n",
            "Iteration 96, loss = 1.28973004\n",
            "Iteration 97, loss = 1.28405299\n",
            "Iteration 98, loss = 1.27840602\n",
            "Iteration 99, loss = 1.27278641\n",
            "Iteration 100, loss = 1.26719690\n",
            "Iteration 101, loss = 1.26163867\n",
            "Iteration 102, loss = 1.25610213\n",
            "Iteration 103, loss = 1.25059050\n",
            "Iteration 104, loss = 1.24509986\n",
            "Iteration 105, loss = 1.23962976\n",
            "Iteration 106, loss = 1.23418301\n",
            "Iteration 107, loss = 1.22875864\n",
            "Iteration 108, loss = 1.22335111\n",
            "Iteration 109, loss = 1.21796299\n",
            "Iteration 110, loss = 1.21259104\n",
            "Iteration 111, loss = 1.20723363\n",
            "Iteration 112, loss = 1.20189630\n",
            "Iteration 113, loss = 1.19657883\n",
            "Iteration 114, loss = 1.19128278\n",
            "Iteration 115, loss = 1.18600815\n",
            "Iteration 116, loss = 1.18075527\n",
            "Iteration 117, loss = 1.17552825\n",
            "Iteration 118, loss = 1.17032086\n",
            "Iteration 119, loss = 1.16514009\n",
            "Iteration 120, loss = 1.15998526\n",
            "Iteration 121, loss = 1.15485393\n",
            "Iteration 122, loss = 1.14974565\n",
            "Iteration 123, loss = 1.14466329\n",
            "Iteration 124, loss = 1.13960748\n",
            "Iteration 125, loss = 1.13457888\n",
            "Iteration 126, loss = 1.12957812\n",
            "Iteration 127, loss = 1.12460589\n",
            "Iteration 128, loss = 1.11966286\n",
            "Iteration 129, loss = 1.11474973\n",
            "Iteration 130, loss = 1.10987222\n",
            "Iteration 131, loss = 1.10503677\n",
            "Iteration 132, loss = 1.10023419\n",
            "Iteration 133, loss = 1.09546436\n",
            "Iteration 134, loss = 1.09072772\n",
            "Iteration 135, loss = 1.08602472\n",
            "Iteration 136, loss = 1.08135575\n",
            "Iteration 137, loss = 1.07672122\n",
            "Iteration 138, loss = 1.07212428\n",
            "Iteration 139, loss = 1.06756850\n",
            "Iteration 140, loss = 1.06305116\n",
            "Iteration 141, loss = 1.05857148\n",
            "Iteration 142, loss = 1.05413125\n",
            "Iteration 143, loss = 1.04972809\n",
            "Iteration 144, loss = 1.04536242\n",
            "Iteration 145, loss = 1.04103520\n",
            "Iteration 146, loss = 1.03674716\n",
            "Iteration 147, loss = 1.03249720\n",
            "Iteration 148, loss = 1.02828530\n",
            "Iteration 149, loss = 1.02411130\n",
            "Iteration 150, loss = 1.01997526\n",
            "Iteration 151, loss = 1.01587810\n",
            "Iteration 152, loss = 1.01182093\n",
            "Iteration 153, loss = 1.00780150\n",
            "Iteration 154, loss = 1.00381951\n",
            "Iteration 155, loss = 0.99987494\n",
            "Iteration 156, loss = 0.99596772\n",
            "Iteration 157, loss = 0.99209809\n",
            "Iteration 158, loss = 0.98826631\n",
            "Iteration 159, loss = 0.98447193\n",
            "Iteration 160, loss = 0.98071484\n",
            "Iteration 161, loss = 0.97699487\n",
            "Iteration 162, loss = 0.97331186\n",
            "Iteration 163, loss = 0.96966561\n",
            "Iteration 164, loss = 0.96605591\n",
            "Iteration 165, loss = 0.96248251\n",
            "Iteration 166, loss = 0.95894517\n",
            "Iteration 167, loss = 0.95544361\n",
            "Iteration 168, loss = 0.95198529\n",
            "Iteration 169, loss = 0.94856501\n",
            "Iteration 170, loss = 0.94517989\n",
            "Iteration 171, loss = 0.94182993\n",
            "Iteration 172, loss = 0.93851922\n",
            "Iteration 173, loss = 0.93524379\n",
            "Iteration 174, loss = 0.93200242\n",
            "Iteration 175, loss = 0.92879612\n",
            "Iteration 176, loss = 0.92562516\n",
            "Iteration 177, loss = 0.92248946\n",
            "Iteration 178, loss = 0.91938747\n",
            "Iteration 179, loss = 0.91631911\n",
            "Iteration 180, loss = 0.91328542\n",
            "Iteration 181, loss = 0.91028545\n",
            "Iteration 182, loss = 0.90731809\n",
            "Iteration 183, loss = 0.90438415\n",
            "Iteration 184, loss = 0.90148081\n",
            "Iteration 185, loss = 0.89861120\n",
            "Iteration 186, loss = 0.89577171\n",
            "Iteration 187, loss = 0.89296444\n",
            "Iteration 188, loss = 0.89018725\n",
            "Iteration 189, loss = 0.88744066\n",
            "Iteration 190, loss = 0.88472401\n",
            "Iteration 191, loss = 0.88203697\n",
            "Iteration 192, loss = 0.87937917\n",
            "Iteration 193, loss = 0.87675023\n",
            "Iteration 194, loss = 0.87414996\n",
            "Iteration 195, loss = 0.87157723\n",
            "Iteration 196, loss = 0.86903225\n",
            "Iteration 197, loss = 0.86651441\n",
            "Iteration 198, loss = 0.86402331\n",
            "Iteration 199, loss = 0.86155899\n",
            "Iteration 200, loss = 0.85912050\n",
            "Iteration 201, loss = 0.85670727\n",
            "Iteration 202, loss = 0.85431888\n",
            "Iteration 203, loss = 0.85195488\n",
            "Iteration 204, loss = 0.84961512\n",
            "Iteration 205, loss = 0.84729924\n",
            "Iteration 206, loss = 0.84500662\n",
            "Iteration 207, loss = 0.84273683\n",
            "Iteration 208, loss = 0.84048947\n",
            "Iteration 209, loss = 0.83826413\n",
            "Iteration 210, loss = 0.83606040\n",
            "Iteration 211, loss = 0.83387787\n",
            "Iteration 212, loss = 0.83172133\n",
            "Iteration 213, loss = 0.82958528\n",
            "Iteration 214, loss = 0.82747053\n",
            "Iteration 215, loss = 0.82537725\n",
            "Iteration 216, loss = 0.82330400\n",
            "Iteration 217, loss = 0.82125256\n",
            "Iteration 218, loss = 0.81922011\n",
            "Iteration 219, loss = 0.81720684\n",
            "Iteration 220, loss = 0.81521244\n",
            "Iteration 221, loss = 0.81323645\n",
            "Iteration 222, loss = 0.81127859\n",
            "Iteration 223, loss = 0.80933858\n",
            "Iteration 224, loss = 0.80741614\n",
            "Iteration 225, loss = 0.80551097\n",
            "Iteration 226, loss = 0.80362277\n",
            "Iteration 227, loss = 0.80175121\n",
            "Iteration 228, loss = 0.79989598\n",
            "Iteration 229, loss = 0.79805676\n",
            "Iteration 230, loss = 0.79623321\n",
            "Iteration 231, loss = 0.79442501\n",
            "Iteration 232, loss = 0.79263185\n",
            "Iteration 233, loss = 0.79085341\n",
            "Iteration 234, loss = 0.78908938\n",
            "Iteration 235, loss = 0.78733947\n",
            "Iteration 236, loss = 0.78560393\n",
            "Iteration 237, loss = 0.78388270\n",
            "Iteration 238, loss = 0.78217489\n",
            "Iteration 239, loss = 0.78048014\n",
            "Iteration 240, loss = 0.77879821\n",
            "Iteration 241, loss = 0.77712943\n",
            "Iteration 242, loss = 0.77547348\n",
            "Iteration 243, loss = 0.77382953\n",
            "Iteration 244, loss = 0.77219720\n",
            "Iteration 245, loss = 0.77057626\n",
            "Iteration 246, loss = 0.76896650\n",
            "Iteration 247, loss = 0.76736856\n",
            "Iteration 248, loss = 0.76578526\n",
            "Iteration 249, loss = 0.76421398\n",
            "Iteration 250, loss = 0.76265361\n",
            "Iteration 251, loss = 0.76110389\n",
            "Iteration 252, loss = 0.75956500\n",
            "Iteration 253, loss = 0.75803607\n",
            "Iteration 254, loss = 0.75651691\n",
            "Iteration 255, loss = 0.75500784\n",
            "Iteration 256, loss = 0.75350850\n",
            "Iteration 257, loss = 0.75201871\n",
            "Iteration 258, loss = 0.75053829\n",
            "Iteration 259, loss = 0.74906711\n",
            "Iteration 260, loss = 0.74760498\n",
            "Iteration 261, loss = 0.74615242\n",
            "Iteration 262, loss = 0.74470923\n",
            "Iteration 263, loss = 0.74327432\n",
            "Iteration 264, loss = 0.74184964\n",
            "Iteration 265, loss = 0.74043185\n",
            "Iteration 266, loss = 0.73902279\n",
            "Iteration 267, loss = 0.73762164\n",
            "Iteration 268, loss = 0.73622972\n",
            "Iteration 269, loss = 0.73484424\n",
            "Iteration 270, loss = 0.73346807\n",
            "Iteration 271, loss = 0.73209800\n",
            "Iteration 272, loss = 0.73073700\n",
            "Iteration 273, loss = 0.72938184\n",
            "Iteration 274, loss = 0.72803530\n",
            "Iteration 275, loss = 0.72669762\n",
            "Iteration 276, loss = 0.72536601\n",
            "Iteration 277, loss = 0.72404088\n",
            "Iteration 278, loss = 0.72272209\n",
            "Iteration 279, loss = 0.72140983\n",
            "Iteration 280, loss = 0.72010451\n",
            "Iteration 281, loss = 0.71880564\n",
            "Iteration 282, loss = 0.71751212\n",
            "Iteration 283, loss = 0.71622599\n",
            "Iteration 284, loss = 0.71494491\n",
            "Iteration 285, loss = 0.71366942\n",
            "Iteration 286, loss = 0.71240053\n",
            "Iteration 287, loss = 0.71113626\n",
            "Iteration 288, loss = 0.70987852\n",
            "Iteration 289, loss = 0.70862552\n",
            "Iteration 290, loss = 0.70737765\n",
            "Iteration 291, loss = 0.70613546\n",
            "Iteration 292, loss = 0.70489797\n",
            "Iteration 293, loss = 0.70366537\n",
            "Iteration 294, loss = 0.70243882\n",
            "Iteration 295, loss = 0.70121577\n",
            "Iteration 296, loss = 0.69999791\n",
            "Iteration 297, loss = 0.69878475\n",
            "Iteration 298, loss = 0.69757596\n",
            "Iteration 299, loss = 0.69637185\n",
            "Iteration 300, loss = 0.69517220\n",
            "Iteration 301, loss = 0.69397691\n",
            "Iteration 302, loss = 0.69278575\n",
            "Iteration 303, loss = 0.69159870\n",
            "Iteration 304, loss = 0.69041590\n",
            "Iteration 305, loss = 0.68923696\n",
            "Iteration 306, loss = 0.68806207\n",
            "Iteration 307, loss = 0.68689115\n",
            "Iteration 308, loss = 0.68572397\n",
            "Iteration 309, loss = 0.68456054\n",
            "Iteration 310, loss = 0.68340125\n",
            "Iteration 311, loss = 0.68224505\n",
            "Iteration 312, loss = 0.68109260\n",
            "Iteration 313, loss = 0.67994360\n",
            "Iteration 314, loss = 0.67879799\n",
            "Iteration 315, loss = 0.67765574\n",
            "Iteration 316, loss = 0.67651678\n",
            "Iteration 317, loss = 0.67538110\n",
            "Iteration 318, loss = 0.67424862\n",
            "Iteration 319, loss = 0.67311990\n",
            "Iteration 320, loss = 0.67199387\n",
            "Iteration 321, loss = 0.67087129\n",
            "Iteration 322, loss = 0.66975169\n",
            "Iteration 323, loss = 0.66863503\n",
            "Iteration 324, loss = 0.66752127\n",
            "Iteration 325, loss = 0.66641037\n",
            "Iteration 326, loss = 0.66530228\n",
            "Iteration 327, loss = 0.66419715\n",
            "Iteration 328, loss = 0.66309450\n",
            "Iteration 329, loss = 0.66199470\n",
            "Iteration 330, loss = 0.66089748\n",
            "Iteration 331, loss = 0.65980284\n",
            "Iteration 332, loss = 0.65871072\n",
            "Iteration 333, loss = 0.65762117\n",
            "Iteration 334, loss = 0.65653407\n",
            "Iteration 335, loss = 0.65544938\n",
            "Iteration 336, loss = 0.65436709\n",
            "Iteration 337, loss = 0.65328726\n",
            "Iteration 338, loss = 0.65220967\n",
            "Iteration 339, loss = 0.65113436\n",
            "Iteration 340, loss = 0.65006132\n",
            "Iteration 341, loss = 0.64899057\n",
            "Iteration 342, loss = 0.64792194\n",
            "Iteration 343, loss = 0.64685551\n",
            "Iteration 344, loss = 0.64579126\n",
            "Iteration 345, loss = 0.64472905\n",
            "Iteration 346, loss = 0.64366892\n",
            "Iteration 347, loss = 0.64261092\n",
            "Iteration 348, loss = 0.64155485\n",
            "Iteration 349, loss = 0.64050077\n",
            "Iteration 350, loss = 0.63944878\n",
            "Iteration 351, loss = 0.63839863\n",
            "Iteration 352, loss = 0.63735037\n",
            "Iteration 353, loss = 0.63630419\n",
            "Iteration 354, loss = 0.63525962\n",
            "Iteration 355, loss = 0.63421709\n",
            "Iteration 356, loss = 0.63317635\n",
            "Iteration 357, loss = 0.63213739\n",
            "Iteration 358, loss = 0.63110019\n",
            "Iteration 359, loss = 0.63006479\n",
            "Iteration 360, loss = 0.62903117\n",
            "Iteration 361, loss = 0.62799922\n",
            "Iteration 362, loss = 0.62696897\n",
            "Iteration 363, loss = 0.62594047\n",
            "Iteration 364, loss = 0.62491360\n",
            "Iteration 365, loss = 0.62388838\n",
            "Iteration 366, loss = 0.62286477\n",
            "Iteration 367, loss = 0.62184280\n",
            "Iteration 368, loss = 0.62082249\n",
            "Iteration 369, loss = 0.61980371\n",
            "Iteration 370, loss = 0.61878649\n",
            "Iteration 371, loss = 0.61777081\n",
            "Iteration 372, loss = 0.61675667\n",
            "Iteration 373, loss = 0.61574413\n",
            "Iteration 374, loss = 0.61473302\n",
            "Iteration 375, loss = 0.61372344\n",
            "Iteration 376, loss = 0.61271532\n",
            "Iteration 377, loss = 0.61170867\n",
            "Iteration 378, loss = 0.61070348\n",
            "Iteration 379, loss = 0.60969973\n",
            "Iteration 380, loss = 0.60869751\n",
            "Iteration 381, loss = 0.60769657\n",
            "Iteration 382, loss = 0.60669709\n",
            "Iteration 383, loss = 0.60569901\n",
            "Iteration 384, loss = 0.60470231\n",
            "Iteration 385, loss = 0.60370698\n",
            "Iteration 386, loss = 0.60271301\n",
            "Iteration 387, loss = 0.60172040\n",
            "Iteration 388, loss = 0.60072912\n",
            "Iteration 389, loss = 0.59973919\n",
            "Iteration 390, loss = 0.59875057\n",
            "Iteration 391, loss = 0.59776328\n",
            "Iteration 392, loss = 0.59677728\n",
            "Iteration 393, loss = 0.59579259\n",
            "Iteration 394, loss = 0.59480918\n",
            "Iteration 395, loss = 0.59382705\n",
            "Iteration 396, loss = 0.59284620\n",
            "Iteration 397, loss = 0.59186662\n",
            "Iteration 398, loss = 0.59088898\n",
            "Iteration 399, loss = 0.58991328\n",
            "Iteration 400, loss = 0.58894077\n",
            "Iteration 401, loss = 0.58796853\n",
            "Iteration 402, loss = 0.58699860\n",
            "Iteration 403, loss = 0.58602889\n",
            "Iteration 404, loss = 0.58506090\n",
            "Iteration 405, loss = 0.58409392\n",
            "Iteration 406, loss = 0.58312784\n",
            "Iteration 407, loss = 0.58216426\n",
            "Iteration 408, loss = 0.58120018\n",
            "Iteration 409, loss = 0.58023812\n",
            "Iteration 410, loss = 0.57927783\n",
            "Iteration 411, loss = 0.57831806\n",
            "Iteration 412, loss = 0.57735930\n",
            "Iteration 413, loss = 0.57640164\n",
            "Iteration 414, loss = 0.57544611\n",
            "Iteration 415, loss = 0.57449096\n",
            "Iteration 416, loss = 0.57353681\n",
            "Iteration 417, loss = 0.57258446\n",
            "Iteration 418, loss = 0.57163267\n",
            "Iteration 419, loss = 0.57068201\n",
            "Iteration 420, loss = 0.56973240\n",
            "Iteration 421, loss = 0.56878521\n",
            "Iteration 422, loss = 0.56783770\n",
            "Iteration 423, loss = 0.56689117\n",
            "Iteration 424, loss = 0.56594717\n",
            "Iteration 425, loss = 0.56500283\n",
            "Iteration 426, loss = 0.56406046\n",
            "Iteration 427, loss = 0.56311873\n",
            "Iteration 428, loss = 0.56217803\n",
            "Iteration 429, loss = 0.56123864\n",
            "Iteration 430, loss = 0.56030083\n",
            "Iteration 431, loss = 0.55936324\n",
            "Iteration 432, loss = 0.55842649\n",
            "Iteration 433, loss = 0.55749182\n",
            "Iteration 434, loss = 0.55655776\n",
            "Iteration 435, loss = 0.55562432\n",
            "Iteration 436, loss = 0.55469260\n",
            "Iteration 437, loss = 0.55376131\n",
            "Iteration 438, loss = 0.55283104\n",
            "Iteration 439, loss = 0.55190278\n",
            "Iteration 440, loss = 0.55097458\n",
            "Iteration 441, loss = 0.55004760\n",
            "Iteration 442, loss = 0.54912234\n",
            "Iteration 443, loss = 0.54819717\n",
            "Iteration 444, loss = 0.54727313\n",
            "Iteration 445, loss = 0.54635028\n",
            "Iteration 446, loss = 0.54542921\n",
            "Iteration 447, loss = 0.54450825\n",
            "Iteration 448, loss = 0.54358803\n",
            "Iteration 449, loss = 0.54267025\n",
            "Iteration 450, loss = 0.54175182\n",
            "Iteration 451, loss = 0.54083511\n",
            "Iteration 452, loss = 0.53991997\n",
            "Iteration 453, loss = 0.53900506\n",
            "Iteration 454, loss = 0.53809099\n",
            "Iteration 455, loss = 0.53717938\n",
            "Iteration 456, loss = 0.53626753\n",
            "Iteration 457, loss = 0.53535643\n",
            "Iteration 458, loss = 0.53444784\n",
            "Iteration 459, loss = 0.53353845\n",
            "Iteration 460, loss = 0.53263066\n",
            "Iteration 461, loss = 0.53172524\n",
            "Iteration 462, loss = 0.53081959\n",
            "Iteration 463, loss = 0.52991465\n",
            "Iteration 464, loss = 0.52901087\n",
            "Iteration 465, loss = 0.52810896\n",
            "Iteration 466, loss = 0.52720715\n",
            "Iteration 467, loss = 0.52630669\n",
            "Iteration 468, loss = 0.52540769\n",
            "Iteration 469, loss = 0.52450898\n",
            "Iteration 470, loss = 0.52361140\n",
            "Iteration 471, loss = 0.52271533\n",
            "Iteration 472, loss = 0.52181969\n",
            "Iteration 473, loss = 0.52092549\n",
            "Iteration 474, loss = 0.52003292\n",
            "Iteration 475, loss = 0.51914048\n",
            "Iteration 476, loss = 0.51824879\n",
            "Iteration 477, loss = 0.51735957\n",
            "Iteration 478, loss = 0.51646965\n",
            "Iteration 479, loss = 0.51558163\n",
            "Iteration 480, loss = 0.51469490\n",
            "Iteration 481, loss = 0.51380860\n",
            "Iteration 482, loss = 0.51292350\n",
            "Iteration 483, loss = 0.51204016\n",
            "Iteration 484, loss = 0.51115702\n",
            "Iteration 485, loss = 0.51027472\n",
            "Iteration 486, loss = 0.50939423\n",
            "Iteration 487, loss = 0.50851473\n",
            "Iteration 488, loss = 0.50763576\n",
            "Iteration 489, loss = 0.50675799\n",
            "Iteration 490, loss = 0.50588161\n",
            "Iteration 491, loss = 0.50500569\n",
            "Iteration 492, loss = 0.50413180\n",
            "Iteration 493, loss = 0.50325926\n",
            "Iteration 494, loss = 0.50238753\n",
            "Iteration 495, loss = 0.50151807\n",
            "Iteration 496, loss = 0.50064922\n",
            "Iteration 497, loss = 0.49978149\n",
            "Iteration 498, loss = 0.49891432\n",
            "Iteration 499, loss = 0.49804861\n",
            "Iteration 500, loss = 0.49718434\n",
            "Iteration 501, loss = 0.49632020\n",
            "Iteration 502, loss = 0.49545855\n",
            "Iteration 503, loss = 0.49459670\n",
            "Iteration 504, loss = 0.49373733\n",
            "Iteration 505, loss = 0.49287775\n",
            "Iteration 506, loss = 0.49202038\n",
            "Iteration 507, loss = 0.49116347\n",
            "Iteration 508, loss = 0.49030766\n",
            "Iteration 509, loss = 0.48945381\n",
            "Iteration 510, loss = 0.48860032\n",
            "Iteration 511, loss = 0.48774835\n",
            "Iteration 512, loss = 0.48689748\n",
            "Iteration 513, loss = 0.48604745\n",
            "Iteration 514, loss = 0.48519936\n",
            "Iteration 515, loss = 0.48435154\n",
            "Iteration 516, loss = 0.48350501\n",
            "Iteration 517, loss = 0.48266060\n",
            "Iteration 518, loss = 0.48181610\n",
            "Iteration 519, loss = 0.48097309\n",
            "Iteration 520, loss = 0.48013226\n",
            "Iteration 521, loss = 0.47929126\n",
            "Iteration 522, loss = 0.47845183\n",
            "Iteration 523, loss = 0.47761454\n",
            "Iteration 524, loss = 0.47677757\n",
            "Iteration 525, loss = 0.47594159\n",
            "Iteration 526, loss = 0.47510705\n",
            "Iteration 527, loss = 0.47427392\n",
            "Iteration 528, loss = 0.47344157\n",
            "Iteration 529, loss = 0.47261129\n",
            "Iteration 530, loss = 0.47178119\n",
            "Iteration 531, loss = 0.47095261\n",
            "Iteration 532, loss = 0.47012603\n",
            "Iteration 533, loss = 0.46929994\n",
            "Iteration 534, loss = 0.46847492\n",
            "Iteration 535, loss = 0.46765124\n",
            "Iteration 536, loss = 0.46682921\n",
            "Iteration 537, loss = 0.46600796\n",
            "Iteration 538, loss = 0.46518799\n",
            "Iteration 539, loss = 0.46436978\n",
            "Iteration 540, loss = 0.46355232\n",
            "Iteration 541, loss = 0.46273599\n",
            "Iteration 542, loss = 0.46192150\n",
            "Iteration 543, loss = 0.46110771\n",
            "Iteration 544, loss = 0.46029525\n",
            "Iteration 545, loss = 0.45948416\n",
            "Iteration 546, loss = 0.45867469\n",
            "Iteration 547, loss = 0.45786609\n",
            "Iteration 548, loss = 0.45705866\n",
            "Iteration 549, loss = 0.45625243\n",
            "Iteration 550, loss = 0.45544854\n",
            "Iteration 551, loss = 0.45464437\n",
            "Iteration 552, loss = 0.45384212\n",
            "Iteration 553, loss = 0.45304173\n",
            "Iteration 554, loss = 0.45224200\n",
            "Iteration 555, loss = 0.45144370\n",
            "Iteration 556, loss = 0.45064661\n",
            "Iteration 557, loss = 0.44985074\n",
            "Iteration 558, loss = 0.44905713\n",
            "Iteration 559, loss = 0.44826347\n",
            "Iteration 560, loss = 0.44747165\n",
            "Iteration 561, loss = 0.44668118\n",
            "Iteration 562, loss = 0.44589245\n",
            "Iteration 563, loss = 0.44510467\n",
            "Iteration 564, loss = 0.44431812\n",
            "Iteration 565, loss = 0.44353282\n",
            "Iteration 566, loss = 0.44274882\n",
            "Iteration 567, loss = 0.44196689\n",
            "Iteration 568, loss = 0.44118532\n",
            "Iteration 569, loss = 0.44040548\n",
            "Iteration 570, loss = 0.43962693\n",
            "Iteration 571, loss = 0.43885015\n",
            "Iteration 572, loss = 0.43807472\n",
            "Iteration 573, loss = 0.43730043\n",
            "Iteration 574, loss = 0.43652736\n",
            "Iteration 575, loss = 0.43575607\n",
            "Iteration 576, loss = 0.43498592\n",
            "Iteration 577, loss = 0.43421711\n",
            "Iteration 578, loss = 0.43344964\n",
            "Iteration 579, loss = 0.43268423\n",
            "Iteration 580, loss = 0.43191937\n",
            "Iteration 581, loss = 0.43115622\n",
            "Iteration 582, loss = 0.43039442\n",
            "Iteration 583, loss = 0.42963399\n",
            "Iteration 584, loss = 0.42887508\n",
            "Iteration 585, loss = 0.42811765\n",
            "Iteration 586, loss = 0.42736149\n",
            "Iteration 587, loss = 0.42660669\n",
            "Iteration 588, loss = 0.42585325\n",
            "Iteration 589, loss = 0.42510119\n",
            "Iteration 590, loss = 0.42435051\n",
            "Iteration 591, loss = 0.42360123\n",
            "Iteration 592, loss = 0.42285360\n",
            "Iteration 593, loss = 0.42210721\n",
            "Iteration 594, loss = 0.42136227\n",
            "Iteration 595, loss = 0.42061872\n",
            "Iteration 596, loss = 0.41987656\n",
            "Iteration 597, loss = 0.41913580\n",
            "Iteration 598, loss = 0.41839645\n",
            "Iteration 599, loss = 0.41765885\n",
            "Iteration 600, loss = 0.41692236\n",
            "Iteration 601, loss = 0.41618741\n",
            "Iteration 602, loss = 0.41545385\n",
            "Iteration 603, loss = 0.41472171\n",
            "Iteration 604, loss = 0.41399098\n",
            "Iteration 605, loss = 0.41326169\n",
            "Iteration 606, loss = 0.41253428\n",
            "Iteration 607, loss = 0.41180775\n",
            "Iteration 608, loss = 0.41108291\n",
            "Iteration 609, loss = 0.41035949\n",
            "Iteration 610, loss = 0.40963750\n",
            "Iteration 611, loss = 0.40891695\n",
            "Iteration 612, loss = 0.40819786\n",
            "Iteration 613, loss = 0.40748023\n",
            "Iteration 614, loss = 0.40676433\n",
            "Iteration 615, loss = 0.40604972\n",
            "Iteration 616, loss = 0.40533656\n",
            "Iteration 617, loss = 0.40462485\n",
            "Iteration 618, loss = 0.40391459\n",
            "Iteration 619, loss = 0.40320581\n",
            "Iteration 620, loss = 0.40249850\n",
            "Iteration 621, loss = 0.40179267\n",
            "Iteration 622, loss = 0.40108832\n",
            "Iteration 623, loss = 0.40038576\n",
            "Iteration 624, loss = 0.39968427\n",
            "Iteration 625, loss = 0.39898446\n",
            "Iteration 626, loss = 0.39828613\n",
            "Iteration 627, loss = 0.39758927\n",
            "Iteration 628, loss = 0.39689389\n",
            "Iteration 629, loss = 0.39620000\n",
            "Iteration 630, loss = 0.39550761\n",
            "Iteration 631, loss = 0.39481673\n",
            "Iteration 632, loss = 0.39412738\n",
            "Iteration 633, loss = 0.39343952\n",
            "Iteration 634, loss = 0.39275350\n",
            "Iteration 635, loss = 0.39206860\n",
            "Iteration 636, loss = 0.39138539\n",
            "Iteration 637, loss = 0.39070367\n",
            "Iteration 638, loss = 0.39002342\n",
            "Iteration 639, loss = 0.38934476\n",
            "Iteration 640, loss = 0.38866761\n",
            "Iteration 641, loss = 0.38799196\n",
            "Iteration 642, loss = 0.38731795\n",
            "Iteration 643, loss = 0.38664532\n",
            "Iteration 644, loss = 0.38597430\n",
            "Iteration 645, loss = 0.38530482\n",
            "Iteration 646, loss = 0.38463685\n",
            "Iteration 647, loss = 0.38397041\n",
            "Iteration 648, loss = 0.38330550\n",
            "Iteration 649, loss = 0.38264212\n",
            "Iteration 650, loss = 0.38198033\n",
            "Iteration 651, loss = 0.38131999\n",
            "Iteration 652, loss = 0.38066122\n",
            "Iteration 653, loss = 0.38000398\n",
            "Iteration 654, loss = 0.37934828\n",
            "Iteration 655, loss = 0.37869412\n",
            "Iteration 656, loss = 0.37804152\n",
            "Iteration 657, loss = 0.37739087\n",
            "Iteration 658, loss = 0.37674126\n",
            "Iteration 659, loss = 0.37609305\n",
            "Iteration 660, loss = 0.37544656\n",
            "Iteration 661, loss = 0.37480167\n",
            "Iteration 662, loss = 0.37415832\n",
            "Iteration 663, loss = 0.37351651\n",
            "Iteration 664, loss = 0.37287623\n",
            "Iteration 665, loss = 0.37223749\n",
            "Iteration 666, loss = 0.37160028\n",
            "Iteration 667, loss = 0.37096462\n",
            "Iteration 668, loss = 0.37033050\n",
            "Iteration 669, loss = 0.36969793\n",
            "Iteration 670, loss = 0.36906691\n",
            "Iteration 671, loss = 0.36843744\n",
            "Iteration 672, loss = 0.36780952\n",
            "Iteration 673, loss = 0.36718314\n",
            "Iteration 674, loss = 0.36655832\n",
            "Iteration 675, loss = 0.36593504\n",
            "Iteration 676, loss = 0.36531331\n",
            "Iteration 677, loss = 0.36469312\n",
            "Iteration 678, loss = 0.36407449\n",
            "Iteration 679, loss = 0.36345740\n",
            "Iteration 680, loss = 0.36284186\n",
            "Iteration 681, loss = 0.36222787\n",
            "Iteration 682, loss = 0.36161543\n",
            "Iteration 683, loss = 0.36100453\n",
            "Iteration 684, loss = 0.36039519\n",
            "Iteration 685, loss = 0.35978739\n",
            "Iteration 686, loss = 0.35918114\n",
            "Iteration 687, loss = 0.35857644\n",
            "Iteration 688, loss = 0.35797329\n",
            "Iteration 689, loss = 0.35737168\n",
            "Iteration 690, loss = 0.35677162\n",
            "Iteration 691, loss = 0.35617312\n",
            "Iteration 692, loss = 0.35557619\n",
            "Iteration 693, loss = 0.35498081\n",
            "Iteration 694, loss = 0.35438698\n",
            "Iteration 695, loss = 0.35379469\n",
            "Iteration 696, loss = 0.35320394\n",
            "Iteration 697, loss = 0.35261474\n",
            "Iteration 698, loss = 0.35202709\n",
            "Iteration 699, loss = 0.35144097\n",
            "Iteration 700, loss = 0.35085640\n",
            "Iteration 701, loss = 0.35027337\n",
            "Iteration 702, loss = 0.34969188\n",
            "Iteration 703, loss = 0.34911193\n",
            "Iteration 704, loss = 0.34853351\n",
            "Iteration 705, loss = 0.34795783\n",
            "Iteration 706, loss = 0.34738341\n",
            "Iteration 707, loss = 0.34681050\n",
            "Iteration 708, loss = 0.34623916\n",
            "Iteration 709, loss = 0.34566943\n",
            "Iteration 710, loss = 0.34510177\n",
            "Iteration 711, loss = 0.34453490\n",
            "Iteration 712, loss = 0.34396996\n",
            "Iteration 713, loss = 0.34340658\n",
            "Iteration 714, loss = 0.34284476\n",
            "Iteration 715, loss = 0.34228448\n",
            "Iteration 716, loss = 0.34172574\n",
            "Iteration 717, loss = 0.34116854\n",
            "Iteration 718, loss = 0.34061288\n",
            "Iteration 719, loss = 0.34005875\n",
            "Iteration 720, loss = 0.33950617\n",
            "Iteration 721, loss = 0.33895512\n",
            "Iteration 722, loss = 0.33840562\n",
            "Iteration 723, loss = 0.33785779\n",
            "Iteration 724, loss = 0.33731143\n",
            "Iteration 725, loss = 0.33676655\n",
            "Iteration 726, loss = 0.33622320\n",
            "Iteration 727, loss = 0.33568149\n",
            "Iteration 728, loss = 0.33514124\n",
            "Iteration 729, loss = 0.33460253\n",
            "Iteration 730, loss = 0.33406534\n",
            "Iteration 731, loss = 0.33352970\n",
            "Iteration 732, loss = 0.33299555\n",
            "Iteration 733, loss = 0.33246292\n",
            "Iteration 734, loss = 0.33193180\n",
            "Iteration 735, loss = 0.33140220\n",
            "Iteration 736, loss = 0.33087410\n",
            "Iteration 737, loss = 0.33034750\n",
            "Iteration 738, loss = 0.32982239\n",
            "Iteration 739, loss = 0.32929878\n",
            "Iteration 740, loss = 0.32877673\n",
            "Iteration 741, loss = 0.32825609\n",
            "Iteration 742, loss = 0.32773697\n",
            "Iteration 743, loss = 0.32721935\n",
            "Iteration 744, loss = 0.32670323\n",
            "Iteration 745, loss = 0.32618858\n",
            "Iteration 746, loss = 0.32567540\n",
            "Iteration 747, loss = 0.32516373\n",
            "Iteration 748, loss = 0.32465350\n",
            "Iteration 749, loss = 0.32414477\n",
            "Iteration 750, loss = 0.32363753\n",
            "Iteration 751, loss = 0.32313173\n",
            "Iteration 752, loss = 0.32262741\n",
            "Iteration 753, loss = 0.32212455\n",
            "Iteration 754, loss = 0.32162315\n",
            "Iteration 755, loss = 0.32112322\n",
            "Iteration 756, loss = 0.32062480\n",
            "Iteration 757, loss = 0.32012792\n",
            "Iteration 758, loss = 0.31963290\n",
            "Iteration 759, loss = 0.31913892\n",
            "Iteration 760, loss = 0.31864676\n",
            "Iteration 761, loss = 0.31815583\n",
            "Iteration 762, loss = 0.31766656\n",
            "Iteration 763, loss = 0.31717847\n",
            "Iteration 764, loss = 0.31669204\n",
            "Iteration 765, loss = 0.31620694\n",
            "Iteration 766, loss = 0.31572319\n",
            "Iteration 767, loss = 0.31524129\n",
            "Iteration 768, loss = 0.31476035\n",
            "Iteration 769, loss = 0.31428098\n",
            "Iteration 770, loss = 0.31380326\n",
            "Iteration 771, loss = 0.31332691\n",
            "Iteration 772, loss = 0.31285183\n",
            "Iteration 773, loss = 0.31237811\n",
            "Iteration 774, loss = 0.31190609\n",
            "Iteration 775, loss = 0.31143525\n",
            "Iteration 776, loss = 0.31096600\n",
            "Iteration 777, loss = 0.31049812\n",
            "Iteration 778, loss = 0.31003149\n",
            "Iteration 779, loss = 0.30956647\n",
            "Iteration 780, loss = 0.30910272\n",
            "Iteration 781, loss = 0.30864033\n",
            "Iteration 782, loss = 0.30817970\n",
            "Iteration 783, loss = 0.30772048\n",
            "Iteration 784, loss = 0.30726215\n",
            "Iteration 785, loss = 0.30680560\n",
            "Iteration 786, loss = 0.30635042\n",
            "Iteration 787, loss = 0.30589656\n",
            "Iteration 788, loss = 0.30544430\n",
            "Iteration 789, loss = 0.30499338\n",
            "Iteration 790, loss = 0.30454369\n",
            "Iteration 791, loss = 0.30409539\n",
            "Iteration 792, loss = 0.30364858\n",
            "Iteration 793, loss = 0.30320294\n",
            "Iteration 794, loss = 0.30275902\n",
            "Iteration 795, loss = 0.30231603\n",
            "Iteration 796, loss = 0.30187456\n",
            "Iteration 797, loss = 0.30143458\n",
            "Iteration 798, loss = 0.30099573\n",
            "Iteration 799, loss = 0.30055829\n",
            "Iteration 800, loss = 0.30012223\n",
            "Iteration 801, loss = 0.29968738\n",
            "Iteration 802, loss = 0.29925412\n",
            "Iteration 803, loss = 0.29882193\n",
            "Iteration 804, loss = 0.29839116\n",
            "Iteration 805, loss = 0.29796170\n",
            "Iteration 806, loss = 0.29753351\n",
            "Iteration 807, loss = 0.29710681\n",
            "Iteration 808, loss = 0.29668121\n",
            "Iteration 809, loss = 0.29625705\n",
            "Iteration 810, loss = 0.29583410\n",
            "Iteration 811, loss = 0.29541248\n",
            "Iteration 812, loss = 0.29499226\n",
            "Iteration 813, loss = 0.29457316\n",
            "Iteration 814, loss = 0.29415553\n",
            "Iteration 815, loss = 0.29373899\n",
            "Iteration 816, loss = 0.29332391\n",
            "Iteration 817, loss = 0.29291001\n",
            "Iteration 818, loss = 0.29249735\n",
            "Iteration 819, loss = 0.29208619\n",
            "Iteration 820, loss = 0.29167605\n",
            "Iteration 821, loss = 0.29126729\n",
            "Iteration 822, loss = 0.29085972\n",
            "Iteration 823, loss = 0.29045353\n",
            "Iteration 824, loss = 0.29004848\n",
            "Iteration 825, loss = 0.28964474\n",
            "Iteration 826, loss = 0.28924230\n",
            "Iteration 827, loss = 0.28884094\n",
            "Iteration 828, loss = 0.28844116\n",
            "Iteration 829, loss = 0.28804226\n",
            "Iteration 830, loss = 0.28764481\n",
            "Iteration 831, loss = 0.28724844\n",
            "Iteration 832, loss = 0.28685345\n",
            "Iteration 833, loss = 0.28645954\n",
            "Iteration 834, loss = 0.28606704\n",
            "Iteration 835, loss = 0.28567556\n",
            "Iteration 836, loss = 0.28528548\n",
            "Iteration 837, loss = 0.28489649\n",
            "Iteration 838, loss = 0.28450878\n",
            "Iteration 839, loss = 0.28412228\n",
            "Iteration 840, loss = 0.28373689\n",
            "Iteration 841, loss = 0.28335289\n",
            "Iteration 842, loss = 0.28296986\n",
            "Iteration 843, loss = 0.28258830\n",
            "Iteration 844, loss = 0.28220765\n",
            "Iteration 845, loss = 0.28182839\n",
            "Iteration 846, loss = 0.28145015\n",
            "Iteration 847, loss = 0.28107324\n",
            "Iteration 848, loss = 0.28069734\n",
            "Iteration 849, loss = 0.28032285\n",
            "Iteration 850, loss = 0.27994927\n",
            "Iteration 851, loss = 0.27957714\n",
            "Iteration 852, loss = 0.27920589\n",
            "Iteration 853, loss = 0.27883608\n",
            "Iteration 854, loss = 0.27846717\n",
            "Iteration 855, loss = 0.27809966\n",
            "Iteration 856, loss = 0.27773307\n",
            "Iteration 857, loss = 0.27736851\n",
            "Iteration 858, loss = 0.27700461\n",
            "Iteration 859, loss = 0.27664202\n",
            "Iteration 860, loss = 0.27628056\n",
            "Iteration 861, loss = 0.27592022\n",
            "Iteration 862, loss = 0.27556105\n",
            "Iteration 863, loss = 0.27520312\n",
            "Iteration 864, loss = 0.27484623\n",
            "Iteration 865, loss = 0.27449049\n",
            "Iteration 866, loss = 0.27413596\n",
            "Iteration 867, loss = 0.27378244\n",
            "Iteration 868, loss = 0.27343023\n",
            "Iteration 869, loss = 0.27307892\n",
            "Iteration 870, loss = 0.27272897\n",
            "Iteration 871, loss = 0.27237990\n",
            "Iteration 872, loss = 0.27203200\n",
            "Iteration 873, loss = 0.27168520\n",
            "Iteration 874, loss = 0.27133959\n",
            "Iteration 875, loss = 0.27099492\n",
            "Iteration 876, loss = 0.27065156\n",
            "Iteration 877, loss = 0.27030907\n",
            "Iteration 878, loss = 0.26996792\n",
            "Iteration 879, loss = 0.26962763\n",
            "Iteration 880, loss = 0.26928853\n",
            "Iteration 881, loss = 0.26895042\n",
            "Iteration 882, loss = 0.26861357\n",
            "Iteration 883, loss = 0.26827757\n",
            "Iteration 884, loss = 0.26794287\n",
            "Iteration 885, loss = 0.26760905\n",
            "Iteration 886, loss = 0.26727632\n",
            "Iteration 887, loss = 0.26694469\n",
            "Iteration 888, loss = 0.26661412\n",
            "Iteration 889, loss = 0.26628455\n",
            "Iteration 890, loss = 0.26595611\n",
            "Iteration 891, loss = 0.26562861\n",
            "Iteration 892, loss = 0.26530228\n",
            "Iteration 893, loss = 0.26497683\n",
            "Iteration 894, loss = 0.26465264\n",
            "Iteration 895, loss = 0.26432925\n",
            "Iteration 896, loss = 0.26400701\n",
            "Iteration 897, loss = 0.26368571\n",
            "Iteration 898, loss = 0.26336550\n",
            "Iteration 899, loss = 0.26304645\n",
            "Iteration 900, loss = 0.26272823\n",
            "Iteration 901, loss = 0.26241107\n",
            "Iteration 902, loss = 0.26209493\n",
            "Iteration 903, loss = 0.26177988\n",
            "Iteration 904, loss = 0.26146564\n",
            "Iteration 905, loss = 0.26115250\n",
            "Iteration 906, loss = 0.26084042\n",
            "Iteration 907, loss = 0.26052947\n",
            "Iteration 908, loss = 0.26021916\n",
            "Iteration 909, loss = 0.25991024\n",
            "Iteration 910, loss = 0.25960186\n",
            "Iteration 911, loss = 0.25929468\n",
            "Iteration 912, loss = 0.25898857\n",
            "Iteration 913, loss = 0.25868343\n",
            "Iteration 914, loss = 0.25837927\n",
            "Iteration 915, loss = 0.25807602\n",
            "Iteration 916, loss = 0.25777379\n",
            "Iteration 917, loss = 0.25747251\n",
            "Iteration 918, loss = 0.25717215\n",
            "Iteration 919, loss = 0.25687294\n",
            "Iteration 920, loss = 0.25657441\n",
            "Iteration 921, loss = 0.25627714\n",
            "Iteration 922, loss = 0.25598047\n",
            "Iteration 923, loss = 0.25568503\n",
            "Iteration 924, loss = 0.25539041\n",
            "Iteration 925, loss = 0.25509667\n",
            "Iteration 926, loss = 0.25480401\n",
            "Iteration 927, loss = 0.25451220\n",
            "Iteration 928, loss = 0.25422126\n",
            "Iteration 929, loss = 0.25393137\n",
            "Iteration 930, loss = 0.25364237\n",
            "Iteration 931, loss = 0.25335420\n",
            "Iteration 932, loss = 0.25306697\n",
            "Iteration 933, loss = 0.25278067\n",
            "Iteration 934, loss = 0.25249535\n",
            "Iteration 935, loss = 0.25221083\n",
            "Iteration 936, loss = 0.25192736\n",
            "Iteration 937, loss = 0.25164467\n",
            "Iteration 938, loss = 0.25136287\n",
            "Iteration 939, loss = 0.25108194\n",
            "Iteration 940, loss = 0.25080200\n",
            "Iteration 941, loss = 0.25052292\n",
            "Iteration 942, loss = 0.25024467\n",
            "Iteration 943, loss = 0.24996729\n",
            "Iteration 944, loss = 0.24969087\n",
            "Iteration 945, loss = 0.24941523\n",
            "Iteration 946, loss = 0.24914048\n",
            "Iteration 947, loss = 0.24886672\n",
            "Iteration 948, loss = 0.24859372\n",
            "Iteration 949, loss = 0.24832160\n",
            "Iteration 950, loss = 0.24805032\n",
            "Iteration 951, loss = 0.24777988\n",
            "Iteration 952, loss = 0.24751028\n",
            "Iteration 953, loss = 0.24724165\n",
            "Iteration 954, loss = 0.24697372\n",
            "Iteration 955, loss = 0.24670670\n",
            "Iteration 956, loss = 0.24644052\n",
            "Iteration 957, loss = 0.24617522\n",
            "Iteration 958, loss = 0.24591072\n",
            "Iteration 959, loss = 0.24564705\n",
            "Iteration 960, loss = 0.24538420\n",
            "Iteration 961, loss = 0.24512217\n",
            "Iteration 962, loss = 0.24486097\n",
            "Iteration 963, loss = 0.24460060\n",
            "Iteration 964, loss = 0.24434104\n",
            "Iteration 965, loss = 0.24408229\n",
            "Iteration 966, loss = 0.24382435\n",
            "Iteration 967, loss = 0.24356728\n",
            "Iteration 968, loss = 0.24331094\n",
            "Iteration 969, loss = 0.24305543\n",
            "Iteration 970, loss = 0.24280071\n",
            "Iteration 971, loss = 0.24254679\n",
            "Iteration 972, loss = 0.24229365\n",
            "Iteration 973, loss = 0.24204131\n",
            "Iteration 974, loss = 0.24178976\n",
            "Iteration 975, loss = 0.24153901\n",
            "Iteration 976, loss = 0.24128903\n",
            "Iteration 977, loss = 0.24103983\n",
            "Iteration 978, loss = 0.24079140\n",
            "Iteration 979, loss = 0.24054375\n",
            "Iteration 980, loss = 0.24029687\n",
            "Iteration 981, loss = 0.24005075\n",
            "Iteration 982, loss = 0.23980539\n",
            "Iteration 983, loss = 0.23956080\n",
            "Iteration 984, loss = 0.23931697\n",
            "Iteration 985, loss = 0.23907390\n",
            "Iteration 986, loss = 0.23883158\n",
            "Iteration 987, loss = 0.23859000\n",
            "Iteration 988, loss = 0.23834918\n",
            "Iteration 989, loss = 0.23810910\n",
            "Iteration 990, loss = 0.23786976\n",
            "Iteration 991, loss = 0.23763116\n",
            "Iteration 992, loss = 0.23739330\n",
            "Iteration 993, loss = 0.23715621\n",
            "Iteration 994, loss = 0.23691987\n",
            "Iteration 995, loss = 0.23668424\n",
            "Iteration 996, loss = 0.23644934\n",
            "Iteration 997, loss = 0.23621514\n",
            "Iteration 998, loss = 0.23598181\n",
            "Iteration 999, loss = 0.23574904\n",
            "Iteration 1000, loss = 0.23551703\n",
            "Iteration 1001, loss = 0.23528574\n",
            "Iteration 1002, loss = 0.23505517\n",
            "Iteration 1003, loss = 0.23482539\n",
            "Iteration 1004, loss = 0.23459626\n",
            "Iteration 1005, loss = 0.23436778\n",
            "Iteration 1006, loss = 0.23414004\n",
            "Iteration 1007, loss = 0.23391301\n",
            "Iteration 1008, loss = 0.23368666\n",
            "Iteration 1009, loss = 0.23346108\n",
            "Iteration 1010, loss = 0.23323609\n",
            "Iteration 1011, loss = 0.23301184\n",
            "Iteration 1012, loss = 0.23278831\n",
            "Iteration 1013, loss = 0.23256543\n",
            "Iteration 1014, loss = 0.23234323\n",
            "Iteration 1015, loss = 0.23212170\n",
            "Iteration 1016, loss = 0.23190089\n",
            "Iteration 1017, loss = 0.23168072\n",
            "Iteration 1018, loss = 0.23146122\n",
            "Iteration 1019, loss = 0.23124245\n",
            "Iteration 1020, loss = 0.23102427\n",
            "Iteration 1021, loss = 0.23080678\n",
            "Iteration 1022, loss = 0.23058998\n",
            "Iteration 1023, loss = 0.23037382\n",
            "Iteration 1024, loss = 0.23015832\n",
            "Iteration 1025, loss = 0.22994347\n",
            "Iteration 1026, loss = 0.22972927\n",
            "Iteration 1027, loss = 0.22951573\n",
            "Iteration 1028, loss = 0.22930285\n",
            "Iteration 1029, loss = 0.22909060\n",
            "Iteration 1030, loss = 0.22887899\n",
            "Iteration 1031, loss = 0.22866802\n",
            "Iteration 1032, loss = 0.22845769\n",
            "Iteration 1033, loss = 0.22824800\n",
            "Iteration 1034, loss = 0.22803894\n",
            "Iteration 1035, loss = 0.22783054\n",
            "Iteration 1036, loss = 0.22762273\n",
            "Iteration 1037, loss = 0.22741556\n",
            "Iteration 1038, loss = 0.22720902\n",
            "Iteration 1039, loss = 0.22700310\n",
            "Iteration 1040, loss = 0.22679779\n",
            "Iteration 1041, loss = 0.22659310\n",
            "Iteration 1042, loss = 0.22638903\n",
            "Iteration 1043, loss = 0.22618557\n",
            "Iteration 1044, loss = 0.22598272\n",
            "Iteration 1045, loss = 0.22578048\n",
            "Iteration 1046, loss = 0.22557884\n",
            "Iteration 1047, loss = 0.22537781\n",
            "Iteration 1048, loss = 0.22517737\n",
            "Iteration 1049, loss = 0.22497754\n",
            "Iteration 1050, loss = 0.22477832\n",
            "Iteration 1051, loss = 0.22457968\n",
            "Iteration 1052, loss = 0.22438164\n",
            "Iteration 1053, loss = 0.22418418\n",
            "Iteration 1054, loss = 0.22398731\n",
            "Iteration 1055, loss = 0.22379102\n",
            "Iteration 1056, loss = 0.22359532\n",
            "Iteration 1057, loss = 0.22340019\n",
            "Iteration 1058, loss = 0.22320567\n",
            "Iteration 1059, loss = 0.22301170\n",
            "Iteration 1060, loss = 0.22281831\n",
            "Iteration 1061, loss = 0.22262549\n",
            "Iteration 1062, loss = 0.22243324\n",
            "Iteration 1063, loss = 0.22224159\n",
            "Iteration 1064, loss = 0.22205047\n",
            "Iteration 1065, loss = 0.22185993\n",
            "Iteration 1066, loss = 0.22166994\n",
            "Iteration 1067, loss = 0.22148052\n",
            "Iteration 1068, loss = 0.22129168\n",
            "Iteration 1069, loss = 0.22110337\n",
            "Iteration 1070, loss = 0.22091560\n",
            "Iteration 1071, loss = 0.22072841\n",
            "Iteration 1072, loss = 0.22054176\n",
            "Iteration 1073, loss = 0.22035565\n",
            "Iteration 1074, loss = 0.22017010\n",
            "Iteration 1075, loss = 0.21998509\n",
            "Iteration 1076, loss = 0.21980061\n",
            "Iteration 1077, loss = 0.21961669\n",
            "Iteration 1078, loss = 0.21943334\n",
            "Iteration 1079, loss = 0.21925051\n",
            "Iteration 1080, loss = 0.21906816\n",
            "Iteration 1081, loss = 0.21888637\n",
            "Iteration 1082, loss = 0.21870512\n",
            "Iteration 1083, loss = 0.21852440\n",
            "Iteration 1084, loss = 0.21834422\n",
            "Iteration 1085, loss = 0.21816455\n",
            "Iteration 1086, loss = 0.21798540\n",
            "Iteration 1087, loss = 0.21780679\n",
            "Iteration 1088, loss = 0.21762868\n",
            "Iteration 1089, loss = 0.21745109\n",
            "Iteration 1090, loss = 0.21727403\n",
            "Iteration 1091, loss = 0.21709746\n",
            "Iteration 1092, loss = 0.21692139\n",
            "Iteration 1093, loss = 0.21674588\n",
            "Iteration 1094, loss = 0.21657083\n",
            "Iteration 1095, loss = 0.21639628\n",
            "Iteration 1096, loss = 0.21622227\n",
            "Iteration 1097, loss = 0.21604873\n",
            "Iteration 1098, loss = 0.21587573\n",
            "Iteration 1099, loss = 0.21570317\n",
            "Iteration 1100, loss = 0.21553115\n",
            "Iteration 1101, loss = 0.21535963\n",
            "Iteration 1102, loss = 0.21518856\n",
            "Iteration 1103, loss = 0.21501797\n",
            "Iteration 1104, loss = 0.21484800\n",
            "Iteration 1105, loss = 0.21467834\n",
            "Iteration 1106, loss = 0.21450923\n",
            "Iteration 1107, loss = 0.21434063\n",
            "Iteration 1108, loss = 0.21417252\n",
            "Iteration 1109, loss = 0.21400486\n",
            "Iteration 1110, loss = 0.21383766\n",
            "Iteration 1111, loss = 0.21367096\n",
            "Iteration 1112, loss = 0.21350473\n",
            "Iteration 1113, loss = 0.21333897\n",
            "Iteration 1114, loss = 0.21317368\n",
            "Iteration 1115, loss = 0.21300886\n",
            "Iteration 1116, loss = 0.21284451\n",
            "Iteration 1117, loss = 0.21268063\n",
            "Iteration 1118, loss = 0.21251719\n",
            "Iteration 1119, loss = 0.21235425\n",
            "Iteration 1120, loss = 0.21219173\n",
            "Iteration 1121, loss = 0.21202971\n",
            "Iteration 1122, loss = 0.21186810\n",
            "Iteration 1123, loss = 0.21170699\n",
            "Iteration 1124, loss = 0.21154630\n",
            "Iteration 1125, loss = 0.21138605\n",
            "Iteration 1126, loss = 0.21122630\n",
            "Iteration 1127, loss = 0.21106695\n",
            "Iteration 1128, loss = 0.21090807\n",
            "Iteration 1129, loss = 0.21074963\n",
            "Iteration 1130, loss = 0.21059162\n",
            "Iteration 1131, loss = 0.21043408\n",
            "Iteration 1132, loss = 0.21027695\n",
            "Iteration 1133, loss = 0.21012027\n",
            "Iteration 1134, loss = 0.20996404\n",
            "Iteration 1135, loss = 0.20980823\n",
            "Iteration 1136, loss = 0.20965285\n",
            "Iteration 1137, loss = 0.20949790\n",
            "Iteration 1138, loss = 0.20934340\n",
            "Iteration 1139, loss = 0.20918931\n",
            "Iteration 1140, loss = 0.20903565\n",
            "Iteration 1141, loss = 0.20888243\n",
            "Iteration 1142, loss = 0.20872962\n",
            "Iteration 1143, loss = 0.20857723\n",
            "Iteration 1144, loss = 0.20842526\n",
            "Iteration 1145, loss = 0.20827372\n",
            "Iteration 1146, loss = 0.20812260\n",
            "Iteration 1147, loss = 0.20797189\n",
            "Iteration 1148, loss = 0.20782159\n",
            "Iteration 1149, loss = 0.20767170\n",
            "Iteration 1150, loss = 0.20752223\n",
            "Iteration 1151, loss = 0.20737318\n",
            "Iteration 1152, loss = 0.20722452\n",
            "Iteration 1153, loss = 0.20707628\n",
            "Iteration 1154, loss = 0.20692844\n",
            "Iteration 1155, loss = 0.20678100\n",
            "Iteration 1156, loss = 0.20663397\n",
            "Iteration 1157, loss = 0.20648734\n",
            "Iteration 1158, loss = 0.20634111\n",
            "Iteration 1159, loss = 0.20619528\n",
            "Iteration 1160, loss = 0.20604984\n",
            "Iteration 1161, loss = 0.20590480\n",
            "Iteration 1162, loss = 0.20576016\n",
            "Iteration 1163, loss = 0.20561590\n",
            "Iteration 1164, loss = 0.20547204\n",
            "Iteration 1165, loss = 0.20532857\n",
            "Iteration 1166, loss = 0.20518548\n",
            "Iteration 1167, loss = 0.20504279\n",
            "Iteration 1168, loss = 0.20490047\n",
            "Iteration 1169, loss = 0.20475855\n",
            "Iteration 1170, loss = 0.20461700\n",
            "Iteration 1171, loss = 0.20447584\n",
            "Iteration 1172, loss = 0.20433506\n",
            "Iteration 1173, loss = 0.20419465\n",
            "Iteration 1174, loss = 0.20405463\n",
            "Iteration 1175, loss = 0.20391498\n",
            "Iteration 1176, loss = 0.20377570\n",
            "Iteration 1177, loss = 0.20363679\n",
            "Iteration 1178, loss = 0.20349826\n",
            "Iteration 1179, loss = 0.20336010\n",
            "Iteration 1180, loss = 0.20322231\n",
            "Iteration 1181, loss = 0.20308488\n",
            "Iteration 1182, loss = 0.20294783\n",
            "Iteration 1183, loss = 0.20281113\n",
            "Iteration 1184, loss = 0.20267480\n",
            "Iteration 1185, loss = 0.20253883\n",
            "Iteration 1186, loss = 0.20240323\n",
            "Iteration 1187, loss = 0.20226798\n",
            "Iteration 1188, loss = 0.20213309\n",
            "Iteration 1189, loss = 0.20199856\n",
            "Iteration 1190, loss = 0.20186438\n",
            "Iteration 1191, loss = 0.20173056\n",
            "Iteration 1192, loss = 0.20159709\n",
            "Iteration 1193, loss = 0.20146397\n",
            "Iteration 1194, loss = 0.20133121\n",
            "Iteration 1195, loss = 0.20119879\n",
            "Iteration 1196, loss = 0.20106672\n",
            "Iteration 1197, loss = 0.20093500\n",
            "Iteration 1198, loss = 0.20080362\n",
            "Iteration 1199, loss = 0.20067259\n",
            "Iteration 1200, loss = 0.20054190\n",
            "Iteration 1201, loss = 0.20041155\n",
            "Iteration 1202, loss = 0.20028154\n",
            "Iteration 1203, loss = 0.20015188\n",
            "Iteration 1204, loss = 0.20002254\n",
            "Iteration 1205, loss = 0.19989355\n",
            "Iteration 1206, loss = 0.19976489\n",
            "Iteration 1207, loss = 0.19963657\n",
            "Iteration 1208, loss = 0.19950858\n",
            "Iteration 1209, loss = 0.19938092\n",
            "Iteration 1210, loss = 0.19925359\n",
            "Iteration 1211, loss = 0.19912659\n",
            "Iteration 1212, loss = 0.19899992\n",
            "Iteration 1213, loss = 0.19887358\n",
            "Iteration 1214, loss = 0.19874756\n",
            "Iteration 1215, loss = 0.19862187\n",
            "Iteration 1216, loss = 0.19849650\n",
            "Iteration 1217, loss = 0.19837146\n",
            "Iteration 1218, loss = 0.19824673\n",
            "Iteration 1219, loss = 0.19812233\n",
            "Iteration 1220, loss = 0.19799824\n",
            "Iteration 1221, loss = 0.19787448\n",
            "Iteration 1222, loss = 0.19775103\n",
            "Iteration 1223, loss = 0.19762789\n",
            "Iteration 1224, loss = 0.19750507\n",
            "Iteration 1225, loss = 0.19738256\n",
            "Iteration 1226, loss = 0.19726037\n",
            "Iteration 1227, loss = 0.19713849\n",
            "Iteration 1228, loss = 0.19701691\n",
            "Iteration 1229, loss = 0.19689565\n",
            "Iteration 1230, loss = 0.19677469\n",
            "Iteration 1231, loss = 0.19665404\n",
            "Iteration 1232, loss = 0.19653370\n",
            "Iteration 1233, loss = 0.19641366\n",
            "Iteration 1234, loss = 0.19629392\n",
            "Iteration 1235, loss = 0.19617449\n",
            "Iteration 1236, loss = 0.19605536\n",
            "Iteration 1237, loss = 0.19593653\n",
            "Iteration 1238, loss = 0.19581800\n",
            "Iteration 1239, loss = 0.19569976\n",
            "Iteration 1240, loss = 0.19558183\n",
            "Iteration 1241, loss = 0.19546419\n",
            "Iteration 1242, loss = 0.19534684\n",
            "Iteration 1243, loss = 0.19522979\n",
            "Iteration 1244, loss = 0.19511303\n",
            "Iteration 1245, loss = 0.19499656\n",
            "Iteration 1246, loss = 0.19488039\n",
            "Iteration 1247, loss = 0.19476450\n",
            "Iteration 1248, loss = 0.19464891\n",
            "Iteration 1249, loss = 0.19453360\n",
            "Iteration 1250, loss = 0.19441858\n",
            "Iteration 1251, loss = 0.19430384\n",
            "Iteration 1252, loss = 0.19418939\n",
            "Iteration 1253, loss = 0.19407522\n",
            "Iteration 1254, loss = 0.19396134\n",
            "Iteration 1255, loss = 0.19384774\n",
            "Iteration 1256, loss = 0.19373442\n",
            "Iteration 1257, loss = 0.19362138\n",
            "Iteration 1258, loss = 0.19350861\n",
            "Iteration 1259, loss = 0.19339613\n",
            "Iteration 1260, loss = 0.19328392\n",
            "Iteration 1261, loss = 0.19317199\n",
            "Iteration 1262, loss = 0.19306034\n",
            "Iteration 1263, loss = 0.19294895\n",
            "Iteration 1264, loss = 0.19283785\n",
            "Iteration 1265, loss = 0.19272701\n",
            "Iteration 1266, loss = 0.19261645\n",
            "Iteration 1267, loss = 0.19250615\n",
            "Iteration 1268, loss = 0.19239613\n",
            "Iteration 1269, loss = 0.19228637\n",
            "Iteration 1270, loss = 0.19217688\n",
            "Iteration 1271, loss = 0.19206766\n",
            "Iteration 1272, loss = 0.19195870\n",
            "Iteration 1273, loss = 0.19185001\n",
            "Iteration 1274, loss = 0.19174159\n",
            "Iteration 1275, loss = 0.19163342\n",
            "Iteration 1276, loss = 0.19152552\n",
            "Iteration 1277, loss = 0.19141788\n",
            "Iteration 1278, loss = 0.19131050\n",
            "Iteration 1279, loss = 0.19120338\n",
            "Iteration 1280, loss = 0.19109652\n",
            "Iteration 1281, loss = 0.19098991\n",
            "Iteration 1282, loss = 0.19088356\n",
            "Iteration 1283, loss = 0.19077747\n",
            "Iteration 1284, loss = 0.19067164\n",
            "Iteration 1285, loss = 0.19056605\n",
            "Iteration 1286, loss = 0.19046072\n",
            "Iteration 1287, loss = 0.19035565\n",
            "Iteration 1288, loss = 0.19025082\n",
            "Iteration 1289, loss = 0.19014625\n",
            "Iteration 1290, loss = 0.19004192\n",
            "Iteration 1291, loss = 0.18993785\n",
            "Iteration 1292, loss = 0.18983402\n",
            "Iteration 1293, loss = 0.18973044\n",
            "Iteration 1294, loss = 0.18962711\n",
            "Iteration 1295, loss = 0.18952402\n",
            "Iteration 1296, loss = 0.18942118\n",
            "Iteration 1297, loss = 0.18931858\n",
            "Iteration 1298, loss = 0.18921622\n",
            "Iteration 1299, loss = 0.18911411\n",
            "Iteration 1300, loss = 0.18901224\n",
            "Iteration 1301, loss = 0.18891061\n",
            "Iteration 1302, loss = 0.18880922\n",
            "Iteration 1303, loss = 0.18870806\n",
            "Iteration 1304, loss = 0.18860715\n",
            "Iteration 1305, loss = 0.18850647\n",
            "Iteration 1306, loss = 0.18840603\n",
            "Iteration 1307, loss = 0.18830583\n",
            "Iteration 1308, loss = 0.18820586\n",
            "Iteration 1309, loss = 0.18810613\n",
            "Iteration 1310, loss = 0.18800663\n",
            "Iteration 1311, loss = 0.18790736\n",
            "Iteration 1312, loss = 0.18780833\n",
            "Iteration 1313, loss = 0.18770952\n",
            "Iteration 1314, loss = 0.18761095\n",
            "Iteration 1315, loss = 0.18751260\n",
            "Iteration 1316, loss = 0.18741449\n",
            "Iteration 1317, loss = 0.18731660\n",
            "Iteration 1318, loss = 0.18721894\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trgh5yR32Dv_",
        "colab_type": "code",
        "outputId": "423fd99d-04ac-4a96-ff35-31e1715bdcdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#scikit for machine learning reporting\n",
        "from sklearn.metrics import mean_squared_error \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(classification_report(y_test,y_pred)) # Print summary report\n",
        "print(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))) # Print Confusion matrix \n",
        "print('accuracy is ',accuracy_score(y_pred,y_test)) # Print accuracy score"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       1.00      0.94      0.97        16\n",
            "           2       0.92      1.00      0.96        11\n",
            "\n",
            "   micro avg       0.97      0.97      0.97        40\n",
            "   macro avg       0.97      0.98      0.97        40\n",
            "weighted avg       0.98      0.97      0.98        40\n",
            " samples avg       0.97      0.97      0.97        40\n",
            "\n",
            "[[13  0  0]\n",
            " [ 0 15  1]\n",
            " [ 0  0 11]]\n",
            "accuracy is  0.975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8Jpd4MA2nLY",
        "colab_type": "code",
        "outputId": "b7c10c51-32f4-4da6-d4cd-89e84949ca61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(h.loss_curve_)\n",
        "plt.title('Loss History')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Loss'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f921003b710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxddZ3/8dcnNze52Zo0bbqmbVq6\nsHaB2hYYkF9RREbAhZlBmAroiMwiuIwO6O/njMzoDOrMKLgACopMBVQQEXDYQVEopNiWttCVLumW\nNG2zNHvy+f1xTtLbNG3SZrlL3s/H4z5y7jnf3PO5p+n7nvu93/s95u6IiEjqy0h0ASIiMjAU6CIi\naUKBLiKSJhToIiJpQoEuIpImFOgiImlCgS7SCzO72syeTnQdIr1RoEtSMLMtZvaeBOz3WjN7+Vj1\nuPtSd7+oD4/1EzP7t8GoU6QvFOgiScLMIomuQVKbAl2Snpl90sw2mtk+M3vMzCaE683M/tvMKs2s\n1szeNLPTw22XmNlaM6szsx1m9o/92H/XWfzR9mlm1wNXA180s3oz+03Y/hQze9HMDpjZGjO7LO5x\nf2JmPzCzJ83sIPA5M9sTH+xm9mEzW3mitcvwokCXpGZmi4F/B/4SGA9sBR4MN18EnA/MBArDNtXh\ntnuAT7l7AXA68PwAldTjPt39bmAp8A13z3f3S80sCvwGeBoYA3waWGpms+Ie7yrga0ABcEdYf3z3\nzhLgpwNUu6Q5Bboku6uBe939DXdvBm4BzjazMqCVIAhPBszd33L3XeHvtQKnmtkId9/v7m8cYx+L\nwjPorhsw+Shtj7XPIx4XyAf+w91b3P154HHgo3Ftfu3uf3D3DndvAu4D/hrAzIqB9wE/O0btIl0U\n6JLsJhCclQPg7vUEZ7ETw4D8LvA9oNLM7jazEWHTjwCXAFvN7CUzO/sY+3jV3Yvib8C2nhr2ss+e\nat/u7h1x67YCE+Pub+/2O/8DXGpmeQRn/78/xguGyGEU6JLsdgJTOu+EQTcK2AHg7re7+1nAqQTd\nIF8I17/u7pcTdHU8Cvx8oAo62j6B7lOX7gQmmVn8/7PJnbX39DvuvgN4BfgwQXfL/QNVt6Q/Bbok\nk6iZxeJumcADwHVmNtfMsoGvA8vcfYuZvcvMFoZ91QeBJqDDzLLCseOF7t4K1AIdR93rcTjaPsPN\ne4Bpcc2XAQ0EH5RGzewC4FIOfQZwND8FvgicATwyEHXL8KBAl2TyJNAYd/sXd38W+H/Aw8Au4CTg\nyrD9COCHwH6Croxq4JvhtiXAFjOrBW4g6IsfCMfa5z0E/fYHzOxRd28hCPD3A3uB7wMfc/e3e9nH\nrwjelfzK3RsGqG4ZBkwXuBBJPma2iWCUzrOJrkVSh87QRZKMmX2EoG99oIZayjCRmegCROQQM3uR\n4MPWJd1Gx4j0Sl0uIiJpQl0uIiJpImFdLqNHj/aysrJE7V5EJCUtX758r7uX9LQtYYFeVlZGeXl5\nonYvIpKSzGzr0bapy0VEJE30GujhN/ZeM7OV4fSfX+2hzbVmVmVmK8Lb3wxOuSIicjR96XJpBha7\ne334deeXzey37v5qt3YPufs/DHyJIiLSF70GugfjGuvDu9HwprGOIpIwra2tVFRU0NTUlOhSBk0s\nFqO0tJRoNNrn3+nTh6LhFVSWA9OB77n7sh6afcTMzgfWA5919+7TghJe1eV6gMmTjzbdtIjIsVVU\nVFBQUEBZWRlmluhyBpy7U11dTUVFBVOnTu3z7/XpQ1F3b3f3uUApsKDzMl9xfgOUufts4BmCSfp7\nepy73X2+u88vKelx1I2ISK+ampoYNWpUWoY5gJkxatSo434HclyjXNz9APACcHG39dXh1WQAfgSc\ndVxViIgcp3QN804n8vz6MsqlxMyKwuUc4L3A293ajI+7exnw1nFX0kfrdtfxrafWse9gy2DtQkQk\nJfXlDH088IKZrQJeB55x98fN7Na4K5jfGA5pXAncCFw7OOXCO3vr+e4LG9lTm74fhohI8svPz090\nCUfoyyiXVcC8HtZ/JW75FoKL9w66/OzgE9+6prah2J2ISMpIuW+K5seC16D65tYEVyIicrgtW7aw\nePFiZs+ezYUXXsi2bcG1xn/xi19w+umnM2fOHM4//3wA1qxZw4IFC5g7dy6zZ89mw4YN/d5/ys2H\nXhAGus7QRQTgq79Zw9qdtQP6mKdOGME/X3racf/epz/9aa655hquueYa7r33Xm688UYeffRRbr31\nVp566ikmTpzIgQMHALjzzju56aabuPrqq2lpaaG9vb3fdafcGXpBducZugJdRJLLK6+8wlVXXQXA\nkiVLePnllwE499xzufbaa/nhD3/YFdxnn302X//617ntttvYunUrOTk5/d5/yp2hd3W56AxdROCE\nzqSH2p133smyZct44oknOOuss1i+fDlXXXUVCxcu5IknnuCSSy7hrrvuYvHixf3aT8qdoedEI2SY\nztBFJPmcc845PPjggwAsXbqU8847D4BNmzaxcOFCbr31VkpKSti+fTubN29m2rRp3HjjjVx++eWs\nWrWq3/tPuTN0MyM/O1N96CKSUA0NDZSWlnbd/9znPscdd9zBddddxze/+U1KSkr48Y9/DMAXvvAF\nNmzYgLtz4YUXMmfOHG677Tbuv/9+otEo48aN40tf+lK/a0q5QAcoiEUV6CKSUB0dPV/D+/nnnz9i\n3SOPPHLEuptvvpmbb755QGtKuS4XgJysCE2t/f9EWEQknaRkoMeiGQp0EZFuUjLQc6IRGhXoIsNa\ncKmG9HUizy8lAz2mQBcZ1mKxGNXV1Wkb6p3zocdiseP6vZT8UDQWjVBV19x7QxFJS6WlpVRUVFBV\nVZXoUgZN5xWLjkdKBnpONEJzW8+fMItI+otGo8d1JZ/hIiW7XHKiERpb1OUiIhIvJQM9Fs1QH7qI\nSDepGegahy4icoSUDPTOPvSOjvT8hFtE5ESkZKDHohEAmtp0li4i0iklAz2nM9BbNdJFRKRTSge6\nPhgVETkkJQM9OxqUraGLIiKH9BroZhYzs9fMbKWZrTGzr/bQJtvMHjKzjWa2zMzKBqPYToe6XBTo\nIiKd+nKG3gwsdvc5wFzgYjNb1K3NJ4D97j4d+G/gtoEt83A5WQp0EZHueg10D9SHd6Phrft4wcuB\n+8LlXwIXmpkNWJXdxNSHLiJyhD71oZtZxMxWAJXAM+6+rFuTicB2AHdvA2qAUQNZaLyuD0XVhy4i\n0qVPge7u7e4+FygFFpjZ6SeyMzO73szKzay8P7OkHRqHrmGLIiKdjmuUi7sfAF4ALu62aQcwCcDM\nMoFCoLqH37/b3ee7+/ySkpITq5hgLheAJp2hi4h06csolxIzKwqXc4D3Am93a/YYcE24fAXwvA/i\nzPMahy4icqS+zIc+HrjPzCIELwA/d/fHzexWoNzdHwPuAe43s43APuDKQasYjXIREelJr4Hu7quA\neT2s/0rcchPwFwNb2tHFMnWGLiLSXUp+UzQjw8jK1JzoIiLxUjLQIehH14eiIiKHpGygx6IZmm1R\nRCROygZ6TjSiLhcRkTgpG+gxBbqIyGFSNtBzdF1REZHDpGygxzIV6CIi8VI20HOyIjRolIuISJeU\nDnT1oYuIHJKygZ4bjWj6XBGROKkb6OpyERE5TMoGeixLZ+giIvFSNtBzo5m0tHfQ1q5vi4qIQCoH\nejiFboM+GBURAVI40LvmRFe3i4gIkMKB3nWGrkAXEQEU6CIiaSNlAz3WdV3RtgRXIiKSHFI20HOz\ngqvn6QxdRCSQwoGuLhcRkXgpG+ido1z05SIRkUDKBnrnGbom6BIRCfQa6GY2ycxeMLO1ZrbGzG7q\noc0FZlZjZivC21cGp9xDcqPqQxcRiZfZhzZtwOfd/Q0zKwCWm9kz7r62W7vfu/sHBr7Enh3qctEo\nFxER6MMZurvvcvc3wuU64C1g4mAX1ptoxIhkmM7QRURCx9WHbmZlwDxgWQ+bzzazlWb2WzM77Si/\nf72ZlZtZeVVV1XEX2+2xyI1qCl0RkU59DnQzywceBj7j7rXdNr8BTHH3OcAdwKM9PYa73+3u8919\nfklJyYnW3CVHU+iKiHTpU6CbWZQgzJe6+yPdt7t7rbvXh8tPAlEzGz2glfYgNyui2RZFREJ9GeVi\nwD3AW+7+X0dpMy5sh5ktCB+3eiAL7UlOVqY+FBURCfVllMu5wBLgTTNbEa77EjAZwN3vBK4A/tbM\n2oBG4Ep390Go9zAF2ZnUNyvQRUSgD4Hu7i8D1kub7wLfHaii+qoglsmumqah3q2ISFJK2W+KAozI\niVLb1JroMkREkkJqB3osk7omdbmIiECqB3pOlLqmVjo6Br27XkQk6aV2oMeidDgc1EgXEZEUD/Sc\n4DPdWnW7iIikeKDHogDUNuqDURGR1A70HAW6iEin1A70zjN0dbmIiKR4oHf2oesMXUQktQO9KDcL\ngH0HWxJciYhI4qV0oI+IZRKLZlBZp6//i4ikdKCbGWNHxNhT25zoUkREEi6lAx1gbEGMPbU6QxcR\nSflAHzMim8o6naGLiKR8oAddLk0MwfTrIiJJLQ0CPZuGlnbqdKELERnmUj7QS0fmArCtuiHBlYiI\nJFbKB/q0kjwANu89mOBKREQSK+UDvWxUHmawuao+0aWIiCRUygd6LBphQmEO7+gMXUSGuZQPdAi6\nXTZXKdBFZHjrNdDNbJKZvWBma81sjZnd1EMbM7PbzWyjma0yszMHp9yeTRudx+aqeg1dFJFhrS9n\n6G3A5939VGAR8Pdmdmq3Nu8HZoS364EfDGiVvZgxtoCDLe1U7G8cyt2KiCSVXgPd3Xe5+xvhch3w\nFjCxW7PLgZ964FWgyMzGD3i1R3H6xEIAVu+oGapdiogknePqQzezMmAesKzbponA9rj7FRwZ+pjZ\n9WZWbmblVVVVx1fpMZw8roDMDONNBbqIDGN9DnQzywceBj7j7rUnsjN3v9vd57v7/JKSkhN5iB7F\nohFmjC1QoIvIsNanQDezKEGYL3X3R3posgOYFHe/NFw3ZM6YOILVO2r0waiIDFt9GeViwD3AW+7+\nX0dp9hjwsXC0yyKgxt13DWCdvZpfVsz+hlY2VOoLRiIyPGX2oc25wBLgTTNbEa77EjAZwN3vBJ4E\nLgE2Ag3AdQNf6rGdPW0UAK9sqmbm2IKh3r2ISML1Guju/jJgvbRx4O8HqqgTUToyh4lFOby6uZpr\nzilLZCkiIgmRFt8UheBydGefNIo/bqqmrb0j0eWIiAy5tAl0gAtPHkNNYyvlW/cnuhQRkSGXVoF+\n3swSsiIZPLN2T6JLEREZcmkV6PnZmZwzfRTPvrVHwxdFZNhJq0AHeM8pY9la3cBGDV8UkWEmLQMd\n4Kk1uxNciYjI0Eq7QB9XGOPMyUU88aYCXUSGl7QLdIA/nz2Bt3bVskmXpRORYSQ9A/2MYObeJ1YN\n6ewDIiIJlZaBPq4wxrvKRvL4qp2JLkVEZMikZaADfGD2BNbvqWf9nrpElyIiMiTSNtDff8Y4zOBx\ndbuIyDCRtoE+piDGwqnFPL5yp75kJCLDQtoGOsCH55Wyee9B/rT9QKJLEREZdGkd6O8/YxyxaAYP\nL69IdCkiIoMurQO9IBbl4tPG8ZuVO2lqbU90OSIigyqtAx3gI2eVUtvUxnNvVSa6FBGRQZX2gX7O\nSaMZNyLGw2+o20VE0lvaB3okw/jgvIm8tL6KqrrmRJcjIjJo0j7QAa44ayLtHc6vV+xIdCkiIoNm\nWAT69DEFzCkt5Jca7SIiaWxYBDoEH46+vbuONTtrEl2KiMig6DXQzexeM6s0s9VH2X6BmdWY2Yrw\n9pWBL7P/Lp09gWjEeOQNdbuISHrqyxn6T4CLe2nze3efG95u7X9ZA29kXhYXnjyWX6/YQWt7R6LL\nEREZcL0Gurv/Dtg3BLUMuo+cVcre+hZ+t74q0aWIiAy4gepDP9vMVprZb83stKM1MrPrzazczMqr\nqoY+VC+YVcKovCyNSReRtDQQgf4GMMXd5wB3AI8eraG73+3u8919fklJyQDs+vhEIxlcNncCz66t\nZN/BliHfv4jIYOp3oLt7rbvXh8tPAlEzG93vygbJVQsm09LewU9f2ZLoUkREBlS/A93MxpmZhcsL\nwses7u/jDpYZYwt4zyljuO+PW2hs0YRdIpI++jJs8QHgFWCWmVWY2SfM7AYzuyFscgWw2sxWArcD\nV3qSX1HihnefxP6GVn5evj3RpYiIDJjM3hq4+0d72f5d4LsDVtEQmF9WzPwpI/nBi5v4q3dNIhaN\nJLokEZF+GzbfFO3uH983i921Tfz4D1sSXYqIyIAYtoG+aNooFp88hu+/uJH9GvEiImlg2AY6wBcv\nnsXB5ja+9fS6RJciItJvwzrQTx43gmvOKeNnr23jT9v2J7ocEZF+GdaBDvD5i2YxtiDGl3+1mjbN\n8SIiKWzYB3p+dib/fOmprN1Vyz0vv5PockRETtiwD3SAi08fx/tOG8t/PrOeDXvqEl2OiMgJUaAD\nZsbXPnQG+dmZfP4XKzW9roikJAV6aHR+Nv/2wdNZVVHDD17clOhyRESOmwI9ziVnjOfyuRO4/bkN\nulSdiKQcBXo3X73sNIrzsvjMgys0eZeIpBQFejdFuVn851/OYUNlPf/2xNpElyMi0mcK9B6cN6OE\nT717GkuXbeN/V+9KdDkiIn2iQD+Kz793FrNLC/mnh99k54HGRJcjItIrBfpRZGVmcPuV82hr7+Az\nD66gvSOpp3gXEVGgH0vZ6Dz+9YOn89qWfdzx/IZElyMickwK9F58+MxSPjxvIt95bgMvrqtMdDki\nIkelQO+Dr33oDGaNLeCmB1ewrboh0eWIiPRIgd4HOVkR7l4yH4Dr7y+noaUtwRWJiBxJgd5Hk0fl\n8p0r57JuTx03P/wmSX4dbBEZhhTox+GCWWP4x4tm8djKnXz7WX1IKiLJpddAN7N7zazSzFYfZbuZ\n2e1mttHMVpnZmQNfZvL4uwtO4i/OKuU7z23gode3JbocEZEufTlD/wlw8TG2vx+YEd6uB37Q/7KS\nl5nx9Q+fwfkzS/jSr1bzgka+iEiS6DXQ3f13wL5jNLkc+KkHXgWKzGz8QBWYjKKRDL5/9ZmcPK6A\nv/2f5by6uTrRJYmIDEgf+kRge9z9inDdEczsejMrN7PyqqqqAdh14uRnZ3LfxxcwaWQuH//J67z2\nzrFe80REBt+Qfijq7ne7+3x3n19SUjKUux4Uo/OzWfrJhYwvjHHdj1/j9S0KdRFJnIEI9B3ApLj7\npeG6YWFMQYwHPrmIsSNiLLlnGc+s3ZPokkRkmBqIQH8M+Fg42mURUOPuw2rO2TEjYvzihrOZNbaA\nT91fzgOvafSLiAy9vgxbfAB4BZhlZhVm9gkzu8HMbgibPAlsBjYCPwT+btCqTWKj8rP52ScXcd6M\nEm555E3+/cm3aNPFpkVkCFmivvE4f/58Ly8vT8i+B1Nrewf/8tgali7bxrnTR3H7lfMYlZ+d6LJE\nJE2Y2XJ3n9/TNn1TdIBFIxl87UNn8I0rZvP6lv1cesfLGtYoIkNCgT5I/nL+JB6+4RyyMjP46A9f\n5V8fX0tTqy46LSKDR4E+iM4oLeTJm87jrxdO4Z6X3+HPb/+9ztZFZNAo0AdZblYm//rB0/npxxfQ\n1NrBlXe/yo0P/Ik9tU2JLk1E0owCfYicP7OEZz/3bm5cPJ3/XbObxd96kW8/u566ptZElyYiaUKB\nPoRysiJ87qJZPPPZ8/mzGaP59rMbOO8bL3DnS5tobFH/uoj0j4YtJtCqigP859PreWl9FaPyslhy\n9hSWLJqiYY4iclTHGraoQE8Cr2/Zxw9e3MTzb1eSnZnBh88s5RN/Vsb0MQWJLk1EkowCPUVsrKzj\nnpff4eE3dtDS1sGCsmKuXDCJS84YTywaSXR5IpIEFOgpZm99M79cXsGDr21jS3UDI2KZfGjeRD50\nZilzSgsxs0SXKCIJokBPUR0dzqvvVPPga9v539W7aWnvYMqoXC6fM4HL5k5Ql4zIMKRATwM1ja08\ntWY3j63YyR837aXD4dTxI/jz2eO56NSxTB+TrzN3kWFAgZ5mKuuaeGLVLh5buZM/bTsAwLTRebz3\ntLFcdOo45k0qIiND4S6SjhToaWxPbRNPr93D02t288qmato6nJKCbN5zylj+z6wSzp0+mrzszESX\nKSIDRIE+TNQ0tvLiukqeXrOHF9dVcrClnaxIBu+aOpILZo7hglkl6poRSXEK9GGopa2D8i37eHF9\nFS+uq2T9nnoAJhbl8O5ZJVwwU2fvIqlIgS7sONDIS+uCcP/Dxr0cbGknM8M4c/JIzp0+mnOnj2LO\npCKiEc0GIZLMFOhymJa2Dsq37uN36/fyx017eXNHDe6QlxVh0bRRYcCPZuZYdc+IJJtjBbrebw9D\nWZkZnHPSaM45aTQABxpaeGVTNS9v3MsfN1Xz3NuVAJQUZHPuSYcCfkJRTiLLFpFe6AxdjlCxv4E/\nbqzmD5v28oeNe9lb3wLA5OJcFk4tZuG0USycWsyk4twEVyoy/KjLRU6Yu7NuTx1/2FjNss3VLHtn\nHzWNwRzuE4tyWDitmEVTR7FwWjGTi3PVRSMyyBToMmA6OoKA7wz3197ZR/XB4Ax+3IgYC6cVszAM\n+Gmj8xTwIgOs34FuZhcD3wEiwI/c/T+6bb8W+CawI1z1XXf/0bEeU4GeHtydjZX1vPrOvq6Qr6pr\nBoI++AVlxZw1ZSTzy0ZyyvgRGkUj0k/9+lDUzCLA94D3AhXA62b2mLuv7db0IXf/h35XKynFzJgx\ntoAZYwtYsmgK7s47ew+yLAz417fs54k3dwGQE40wd1IR88tGcuaUkZw5eSSFOdEEPwOR9NGXUS4L\ngI3uvhnAzB4ELge6B7oIZsa0knymleTz0QWTAdhV08jyrfsp37Kf5Vv38/0XN9He4ZjBzDEFnFU2\nkvlTRjJ/SjGTinPUTSNygvoS6BOB7XH3K4CFPbT7iJmdD6wHPuvu27s3MLPrgesBJk+efPzVSkoa\nX5jDB2bn8IHZEwA42NzGyu0HKN+6n/Kt+/nNip38bNk2IOimmT9lJPMmFzGntIgzSgvJzdLoWpG+\nGKj/Kb8BHnD3ZjP7FHAfsLh7I3e/G7gbgj70Adq3pJi87EzOmT6ac6YH4+DbO5z1e+oo37qf5Vv2\nUb51P79dvRuADIOZYwuYO6mIuZOKmDOpiBlj8slUX7zIEfoS6DuASXH3Szn04ScA7l4dd/dHwDf6\nX5oMF5EM45TxIzhl/AiWLJoCBFdtWlVxgBXba1ix/QC/Xb2bB18P3vTlRCOcUVoYBHxpEXMnFzGh\nMKauGhn2+hLorwMzzGwqQZBfCVwV38DMxrv7rvDuZcBbA1qlDDuj87NZfPJYFp88FghG02ytbmDF\n9gOs2H6AlRUH+Mkft9DS1hG2z+K0CYWcPnFE8HNCofrjZdjpNdDdvc3M/gF4imDY4r3uvsbMbgXK\n3f0x4EYzuwxoA/YB1w5izTIMmRllo/MoG53HB+dNBII5ad7eXcvK7cGZ/JqdNfxh417aOoLevIJY\nJqdNGHFY0E8bnafuGklb+mKRpJWm1nbW76ljzc5aVu+oYc3OWt7aVUtzeCYfi2Zw8rgRnDphBCeP\nK2Dm2AJOHldAUW5WgisX6RtNziXDRiwaYXZpEbNLi7rWtbV3sHnvQdbsrGH1jiDoH1+5k58ta+tq\nM6Ygm1njCpg1toCZ44KQnzGmgJysSCKehsgJUaBL2suMZDBzbHA2/qF5wTp3Z09tM+v21LF+dx1v\n765j/Z467n91a9fZvFkwIdnMsQWcVJLPSSV5TAt/6oxekpECXYYlM2NcYYxxhTHePbOka317h7Nt\nXwPrdteybnc96/fUsW5PHS+uq6S1/VD35Ki8LKaV5DFtdD4njQl+TivJY3JxrvroJWEU6CJxIhnG\n1NF5TB2dx8WnH1rf1t7B9v2NbK6qZ3PVQTaFP597ew8Plbd0tYtGjNKRuUwqzmVycQ5TivPC5Vwm\nj8olX5f8k0Gkvy6RPsiMZHQF/YWnHL6tpqGVTXsPBf22fQ1sq25g5fYDXVMNdyrOywrCPbxNKs5h\nQlEO4wtzmFAU07dipV/01yPST4W5Uc6cHEw21l1NQyvb9zewtbohCPp9DWzf18CftgeTlrV3HD7K\nrCg3GoR7YSwI+qIYEwo7Qz/oItKMlXI0CnSRQVSYG6Uwt5DTJxYesa21vYPdNU3sPNDIrpomdhxo\nZFdNI7sOBMvlW/cfcYZvBsW5WZQUZFNSkM2YghhjRmRTkp/NmBHh/XBbnrp3hh39i4skSDSSwaTi\n3GNeyu9gcxu7ahrZeaCp62dVfTOVtc1U1TWxsbKeqrrmri9TxcvLijBmRIyS/GyK87IYmZfFqLif\nxd1usaiGaKY6BbpIEsvLzmT6mAKmjyk4apuODudAYyuVdU1U1jZTWddMVV1zcD9c3lRVz/6tLew7\n2EIP2Q9AblaEkblZjMoPQz43ixE5UUbkRCnsdhuRk9m1nBONaIqFJKFAF0lxGRnWdZZ98rhjt+3o\ncGoaW9nXEIR799v+gy1Uh8sbK+upaWylrqntmI8ZjVgQ8rHDw78glkl+dnDLy84kv9v9gli4PrxF\nMvSi0F8KdJFhJCPDGBl2u5xU0nt7CMbm1zW1UtvYRk1j62G32qZu9xtbOdDQwtbqg9Q3t1HX1Nb1\nRa3e5EQjcUEf6Qr6nKxMcqIZ5GZlEotGyM0Kbp3LOdEIOVkRcrMyu5ZzsiLkhsvZmRnD5h2EAl1E\njimSYRTlZp3wt2Nb2zs42NxGfXg7GAb9weZ26ptbqW9up76pjYMtnesPtd15oImm1nYaWtppbG2n\nsaWdlva+vUB0yjAOC/qcaITszCDos6MZxDIjZEczutbFouG2zAyyO5fj12VGiHW2j2Z0+51gXVYk\nuGUM8bsOBbqIDKpoJKNfLwjdtbV3dIV7Yxj2DS3tXcHf0NIWt9x+2HJjS/COoam1PfzZQU1jK82t\nHTS1tdPc2kFzWwfNbe00tR7fC0dPMjOMrMyM4BbJ6Fq+asFk/ua8aQNwNLrtb8AfUURkEGVGMiiI\nZFAQG9wLjLs7Le1hwLceeuqKxa0AAAbeSURBVBFobms/7EUheBFoP/SzLfidlrYOWto7aA1/toTr\nmts7GJ2fPSg1K9BFRHpgZmE3TARiia6mb/SVMxGRNKFAFxFJEwp0EZE0oUAXEUkTCnQRkTShQBcR\nSRMKdBGRNKFAFxFJE+Z+lLk0B3vHZlXA1hP89dHA3gEsZyilau2qe2ilat2QurWnSt1T3L3HqdUS\nFuj9YWbl7j4/0XWciFStXXUPrVStG1K39lStO566XERE0oQCXUQkTaRqoN+d6AL6IVVrV91DK1Xr\nhtStPVXr7pKSfegiInKkVD1DFxGRbhToIiJpIuUC3cwuNrN1ZrbRzG5OdD3xzGySmb1gZmvNbI2Z\n3RSuLzazZ8xsQ/hzZLjezOz28LmsMrMzE1x/xMz+ZGaPh/enmtmysL6HzCwrXJ8d3t8Ybi9LYM1F\nZvZLM3vbzN4ys7NT6Hh/Nvw7WW1mD5hZLBmPuZnda2aVZrY6bt1xH2MzuyZsv8HMrklQ3d8M/1ZW\nmdmvzKwobtstYd3rzOx9ceuTNnOO4O4pcwMiwCZgGpAFrAROTXRdcfWNB84MlwuA9cCpwDeAm8P1\nNwO3hcuXAL8FDFgELEtw/Z8DfgY8Ht7/OXBluHwn8Lfh8t8Bd4bLVwIPJbDm+4C/CZezgKJUON7A\nROAdICfuWF+bjMccOB84E1gdt+64jjFQDGwOf44Ml0cmoO6LgMxw+ba4uk8N8yQbmBrmTCTZM+eI\n55zoAo7zH+hs4Km4+7cAtyS6rmPU+2vgvcA6YHy4bjywLly+C/hoXPuudgmotRR4DlgMPB7+h9wb\n98ffdeyBp4Czw+XMsJ0loObCMBSt2/pUON4Tge1hwGWGx/x9yXrMgbJuwXhcxxj4KHBX3PrD2g1V\n3d22fQhYGi4fliWdxzvVMifVulw6/xN0qgjXJZ3wLfE8YBkw1t13hZt2A2PD5WR6Pt8Gvgh0Xup8\nFHDA3dvC+/G1ddUdbq8J2w+1qUAV8OOwq+hHZpZHChxvd98BfAvYBuwiOIbLSf5j3ul4j3HSHPs4\nHyd4NwGpVfdRpVqgpwQzywceBj7j7rXx2zx4mU+qsaJm9gGg0t2XJ7qW45RJ8Jb6B+4+DzhI8Pa/\nSzIeb4Cwz/lyghelCUAecHFCizpByXqMj8XMvgy0AUsTXctASrVA3wFMirtfGq5LGmYWJQjzpe7+\nSLh6j5mND7ePByrD9cnyfM4FLjOzLcCDBN0u3wGKzCyzh9q66g63FwLVQ1lwqAKocPdl4f1fEgR8\nsh9vgPcA77h7lbu3Ao8Q/Dsk+zHvdLzHOGmOvZldC3wAuDp8MYIUqLsvUi3QXwdmhCMBsgg+HHos\nwTV1MTMD7gHecvf/itv0GND5qf41BH3rnes/Fo4MWATUxL2NHTLufou7l7p7GcExfd7drwZeAK44\nSt2dz+eKsP2Qn6G5+25gu5nNClddCKwlyY93aBuwyMxyw7+bztqT+pjHOd5j/BRwkZmNDN+dXBSu\nG1JmdjFB1+Jl7t4Qt+kx4MpwNNFUYAbwGkmeOUdIdCf+8d4IPkVfT/DJ85cTXU+32v6M4K3nKmBF\neLuEoK/zOWAD8CxQHLY34Hvhc3kTmJ8Ez+ECDo1ymUbwR70R+AWQHa6Phfc3htunJbDeuUB5eMwf\nJRhBkRLHG/gq8DawGrifYIRF0h1z4AGCfv5WgndFnziRY0zQZ70xvF2XoLo3EvSJd/7/vDOu/ZfD\nutcB749bn7SZ0/2mr/6LiKSJVOtyERGRo1Cgi4ikCQW6iEiaUKCLiKQJBbqISJpQoIucADO7wMJZ\nKUWShQJdRCRNKNAlrZnZX5vZa2a2wszusmDO93oz++9wLvLnzKwkbDvXzF6Nmyu7c47v6Wb2rJmt\nNLM3zOyk8OHz7dBc7EvDb3yKJIwCXdKWmZ0C/BVwrrvPBdqBqwkmwip399OAl4B/Dn/lp8A/ufts\ngm85dq5fCnzP3ecA5xB8+xCC2TQ/QzCX9jSCuVhEEiaz9yYiKetC4Czg9fDkOYdgEqkO4KGwzf8A\nj5hZIVDk7i+F6+8DfmFmBcBEd/8VgLs3AYSP95q7V4T3VxDMvf3y4D8tkZ4p0CWdGXCfu99y2Eqz\n/9et3YnOf9Ect9yO/j9JgqnLRdLZc8AVZjYGuq6DOYXg775zRsOrgJfdvQbYb2bnheuXAC+5ex1Q\nYWYfDB8j28xyh/RZiPSRzigkbbn7WjP7v8DTZpZBMOve3xNcCGNBuK2SoJ8dgmlg7wwDezNwXbh+\nCXCXmd0aPsZfDOHTEOkzzbYow46Z1bt7fqLrEBlo6nIREUkTOkMXEUkTOkMXEUkTCnQRkTShQBcR\nSRMKdBGRNKFAFxFJE/8fDpDgOSNnKb8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}